{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "\n",
    "sales = pd.read_csv('kc_house_data.csv/kc_house_data.csv', dtype=dtype_dict)\n",
    "sales = sales.sort_values(['sqft_living','price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate derivative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data, features, output):\n",
    "    data['constant'] = 1\n",
    "    features = ['constant'] + features   ## why use []\n",
    "    feature_matrix = data[features]\n",
    "    feature_matrix = feature_matrix.to_numpy()\n",
    "    output_array = data[output]\n",
    "    output_array = output_array.to_numpy()\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    predictions = np.dot(feature_matrix,weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    if feature_is_constant is True:\n",
    "        derivative = 2*sum(errors*feature)\n",
    "    else:\n",
    "        derivative = 2*sum(errors*feature)+2*l2_penalty*weight\n",
    "    return(derivative)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-56554166815950.0\n",
      "-56554166815950.0\n",
      "\n",
      "-22446749330.0\n",
      "-22446749330.0\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price')\n",
    "my_weights = np.array([1., 10.])\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "errors = test_predictions - example_output # prediction errors\n",
    "\n",
    "# next two lines should print the same values\n",
    "print (feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False))\n",
    "print (np.sum(errors*example_features[:,1])*2+20.)\n",
    "print ('')\n",
    "\n",
    "# next two lines should print the same values\n",
    "print (feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True))\n",
    "print (np.sum(errors)*2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, inital_weights,\n",
    "                                     setp_size, l2_penalty, max_iterations = 100):\n",
    "    weights = np.array(initial_weights)\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        errors = predictions - output\n",
    "        for i in range(len(weights)):\n",
    "            derivative = feature_derivative_ridge(errors, feature_matrix[:, i], \n",
    "                                     weights[i], l2_penalty, i==0)\n",
    "            weights[i] = weights[i] - step_size*derivative\n",
    "        iteration += 1\n",
    "        print ('Done with gradient descent at iteration ', iteration)\n",
    "        print ('Learned weights = ', str(weights))\n",
    "    return(weights)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Running a simple regression with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kc_house_train_data.csv/kc_house_train_data.csv', dtype=dtype_dict)\n",
    "test_data = pd.read_csv('kc_house_test_data.csv/kc_house_test_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_train_feature_matrix, train_output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-12\n",
    "max_iterations = 1000\n",
    "initial_weights = np.ones(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [ 1.01868035 48.15248648]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [ 1.0339507  86.81965038]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [  1.04642469 118.52846735]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [  1.05660556 144.53112627]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [  1.06490595 165.85447843]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [  1.07166427 183.34058818]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [  1.07715802 197.67998621]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [  1.08161477 209.43893883]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [  1.08522113 219.08180992]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [  1.08813013 226.9893988 ]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  1.09046727 233.47397806]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  1.09233545 238.79162531]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  1.09381907 243.15233571]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  1.09498732 246.72831478]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  1.09589697 249.66077879]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  1.09659455 252.06553145]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  1.09711822 254.03753703]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  1.09749927 255.65467049]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  1.09776338 256.98079282]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  1.09793158 258.06827292]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  1.09802114 258.96005562]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  1.09804621 259.69135765]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  1.09801839 260.29105829]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  1.0979472  260.78283986]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  1.09784044 261.18612293]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  1.09770453 261.51683324]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  1.09754469 261.78803061]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  1.09736524 262.0104247 ]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  1.09716971 262.1927979 ]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  1.09696099 262.34235216]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  1.09674146 262.46499341]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  1.09651305 262.56556478]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  1.09627737 262.64803786]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  1.09603573 262.71566952]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  1.0957892  262.77113055]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  1.09553865 262.81661111]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  1.09528482 262.85390724]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  1.09502829 262.88449176]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  1.09476955 262.90957247]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  1.09450899 262.93013981]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  1.09424695 262.94700596]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  1.09398368 262.96083699]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  1.09371942 262.97217908]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  1.09345434 262.98148012]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  1.09318858 262.98910741]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  1.09292227 262.99536216]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  1.09265551 263.00049135]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  1.09238838 263.00469754]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  1.09212094 263.00814682]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  1.09185326 263.0109754 ]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  1.09158537 263.01329499]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  1.09131731 263.01519718]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  1.09104912 263.01675708]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  1.09078081 263.01803628]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  1.09051241 263.01908531]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  1.09024394 263.01994558]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  1.0899754  263.02065105]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  1.08970681 263.0212296 ]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  1.08943818 263.02170405]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  1.08916951 263.02209314]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  1.08890082 263.02241223]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  1.0886321  263.02267392]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  1.08836337 263.02288853]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  1.08809462 263.02306455]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  1.08782585 263.02320891]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  1.08755708 263.02332731]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  1.0872883  263.02342442]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  1.08701951 263.02350408]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  1.08675071 263.02356942]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  1.08648191 263.02362302]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  1.08621311 263.023667  ]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  1.0859443  263.02370308]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  1.08567549 263.02373269]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  1.08540668 263.02375698]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  1.08513787 263.02377693]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  1.08486905 263.02379331]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  1.08460024 263.02380675]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  1.08433142 263.0238178 ]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  1.0840626  263.02382688]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  1.08379378 263.02383435]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  1.08352496 263.02384049]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  1.08325615 263.02384554]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  1.08298733 263.02384971]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  1.08271851 263.02385314]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  1.08244968 263.02385598]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  1.08218086 263.02385832]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  1.08191204 263.02386027]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  1.08164322 263.02386188]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  1.0813744  263.02386322]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  1.08110558 263.02386434]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  1.08083676 263.02386528]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  1.08056794 263.02386607]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  1.08029912 263.02386673]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  1.0800303 263.0238673]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  1.07976148 263.02386778]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  1.07949265 263.0238682 ]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  1.07922383 263.02386856]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  1.07895501 263.02386887]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  1.07868619 263.02386915]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  1.07841737 263.0238694 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  1.07814855 263.02386962]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  1.07787973 263.02386982]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  1.0776109  263.02387001]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  1.07734208 263.02387018]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  1.07707326 263.02387034]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  1.07680444 263.02387049]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  1.07653562 263.02387063]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  1.0762668  263.02387077]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  1.07599798 263.0238709 ]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  1.07572916 263.02387102]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  1.07546033 263.02387115]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  1.07519151 263.02387127]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  1.07492269 263.02387139]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  1.07465387 263.0238715 ]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  1.07438505 263.02387162]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  1.07411623 263.02387173]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  1.07384741 263.02387185]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  1.07357858 263.02387196]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  1.07330976 263.02387207]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  1.07304094 263.02387218]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  1.07277212 263.02387229]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  1.0725033 263.0238724]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  1.07223448 263.02387251]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  1.07196566 263.02387262]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  1.07169684 263.02387273]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  1.07142801 263.02387284]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  1.07115919 263.02387295]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  1.07089037 263.02387305]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  1.07062155 263.02387316]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  1.07035273 263.02387327]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  1.07008391 263.02387338]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  1.06981509 263.02387349]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  1.06954626 263.0238736 ]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  1.06927744 263.0238737 ]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  1.06900862 263.02387381]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  1.0687398  263.02387392]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  1.06847098 263.02387403]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  1.06820216 263.02387414]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  1.06793334 263.02387424]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  1.06766452 263.02387435]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  1.06739569 263.02387446]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  1.06712687 263.02387457]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  1.06685805 263.02387468]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  1.06658923 263.02387478]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  1.06632041 263.02387489]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  1.06605159 263.023875  ]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  1.06578277 263.02387511]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  1.06551394 263.02387522]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  1.06524512 263.02387532]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  1.0649763  263.02387543]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  1.06470748 263.02387554]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  1.06443866 263.02387565]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  1.06416984 263.02387576]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  1.06390102 263.02387587]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  1.0636322  263.02387597]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  1.06336337 263.02387608]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  1.06309455 263.02387619]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  1.06282573 263.0238763 ]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  1.06255691 263.02387641]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  1.06228809 263.02387651]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  1.06201927 263.02387662]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  1.06175045 263.02387673]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  1.06148163 263.02387684]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  1.0612128  263.02387695]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  1.06094398 263.02387705]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  1.06067516 263.02387716]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  1.06040634 263.02387727]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  1.06013752 263.02387738]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  1.0598687  263.02387749]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  1.05959988 263.02387759]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  1.05933106 263.0238777 ]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  1.05906223 263.02387781]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  1.05879341 263.02387792]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  1.05852459 263.02387803]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  1.05825577 263.02387813]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  1.05798695 263.02387824]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  1.05771813 263.02387835]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  1.05744931 263.02387846]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  1.05718049 263.02387857]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  1.05691166 263.02387867]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  1.05664284 263.02387878]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  1.05637402 263.02387889]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  1.0561052 263.023879 ]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  1.05583638 263.02387911]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  1.05556756 263.02387921]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  1.05529874 263.02387932]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  1.05502992 263.02387943]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  1.05476109 263.02387954]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  1.05449227 263.02387965]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  1.05422345 263.02387975]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  1.05395463 263.02387986]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  1.05368581 263.02387997]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  1.05341699 263.02388008]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  1.05314817 263.02388019]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  1.05287935 263.02388029]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  1.05261052 263.0238804 ]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  1.0523417  263.02388051]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  1.05207288 263.02388062]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  1.05180406 263.02388073]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  1.05153524 263.02388083]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  1.05126642 263.02388094]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  1.0509976  263.02388105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  1.05072878 263.02388116]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  1.05045995 263.02388127]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  1.05019113 263.02388137]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  1.04992231 263.02388148]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  1.04965349 263.02388159]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  1.04938467 263.0238817 ]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  1.04911585 263.02388181]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  1.04884703 263.02388191]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  1.04857821 263.02388202]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  1.04830938 263.02388213]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  1.04804056 263.02388224]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  1.04777174 263.02388235]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  1.04750292 263.02388246]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  1.0472341  263.02388256]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  1.04696528 263.02388267]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  1.04669646 263.02388278]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  1.04642764 263.02388289]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  1.04615881 263.023883  ]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  1.04588999 263.0238831 ]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  1.04562117 263.02388321]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  1.04535235 263.02388332]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  1.04508353 263.02388343]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  1.04481471 263.02388354]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  1.04454589 263.02388364]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  1.04427707 263.02388375]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  1.04400825 263.02388386]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  1.04373942 263.02388397]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  1.0434706  263.02388408]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  1.04320178 263.02388418]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  1.04293296 263.02388429]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  1.04266414 263.0238844 ]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  1.04239532 263.02388451]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  1.0421265  263.02388462]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  1.04185768 263.02388472]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  1.04158885 263.02388483]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  1.04132003 263.02388494]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  1.04105121 263.02388505]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  1.04078239 263.02388516]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  1.04051357 263.02388526]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  1.04024475 263.02388537]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  1.03997593 263.02388548]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  1.03970711 263.02388559]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  1.03943829 263.0238857 ]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  1.03916946 263.0238858 ]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  1.03890064 263.02388591]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  1.03863182 263.02388602]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  1.038363   263.02388613]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  1.03809418 263.02388624]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  1.03782536 263.02388634]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  1.03755654 263.02388645]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  1.03728772 263.02388656]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  1.0370189  263.02388667]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  1.03675007 263.02388678]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  1.03648125 263.02388688]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  1.03621243 263.02388699]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  1.03594361 263.0238871 ]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  1.03567479 263.02388721]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  1.03540597 263.02388732]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  1.03513715 263.02388742]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  1.03486833 263.02388753]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  1.0345995  263.02388764]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  1.03433068 263.02388775]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  1.03406186 263.02388786]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  1.03379304 263.02388796]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  1.03352422 263.02388807]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  1.0332554  263.02388818]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  1.03298658 263.02388829]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  1.03271776 263.0238884 ]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  1.03244894 263.0238885 ]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  1.03218011 263.02388861]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  1.03191129 263.02388872]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  1.03164247 263.02388883]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  1.03137365 263.02388894]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  1.03110483 263.02388904]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  1.03083601 263.02388915]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  1.03056719 263.02388926]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  1.03029837 263.02388937]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  1.03002955 263.02388948]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  1.02976072 263.02388959]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  1.0294919  263.02388969]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  1.02922308 263.0238898 ]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  1.02895426 263.02388991]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  1.02868544 263.02389002]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  1.02841662 263.02389013]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  1.0281478  263.02389023]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  1.02787898 263.02389034]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  1.02761016 263.02389045]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  1.02734134 263.02389056]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  1.02707251 263.02389067]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  1.02680369 263.02389077]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  1.02653487 263.02389088]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  1.02626605 263.02389099]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  1.02599723 263.0238911 ]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  1.02572841 263.02389121]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  1.02545959 263.02389131]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  1.02519077 263.02389142]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  1.02492195 263.02389153]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  1.02465312 263.02389164]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  1.0243843  263.02389175]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  1.02411548 263.02389185]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  1.02384666 263.02389196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  1.02357784 263.02389207]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  1.02330902 263.02389218]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  1.0230402  263.02389229]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  1.02277138 263.02389239]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  1.02250256 263.0238925 ]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  1.02223373 263.02389261]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  1.02196491 263.02389272]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  1.02169609 263.02389283]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  1.02142727 263.02389293]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  1.02115845 263.02389304]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  1.02088963 263.02389315]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  1.02062081 263.02389326]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  1.02035199 263.02389337]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  1.02008317 263.02389347]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  1.01981435 263.02389358]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  1.01954552 263.02389369]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  1.0192767 263.0238938]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  1.01900788 263.02389391]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  1.01873906 263.02389401]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  1.01847024 263.02389412]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  1.01820142 263.02389423]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  1.0179326  263.02389434]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  1.01766378 263.02389445]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  1.01739496 263.02389455]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  1.01712613 263.02389466]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  1.01685731 263.02389477]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  1.01658849 263.02389488]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  1.01631967 263.02389499]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  1.01605085 263.02389509]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  1.01578203 263.0238952 ]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  1.01551321 263.02389531]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  1.01524439 263.02389542]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  1.01497557 263.02389553]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  1.01470675 263.02389563]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  1.01443792 263.02389574]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  1.0141691  263.02389585]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  1.01390028 263.02389596]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  1.01363146 263.02389607]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  1.01336264 263.02389617]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  1.01309382 263.02389628]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  1.012825   263.02389639]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  1.01255618 263.0238965 ]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  1.01228736 263.02389661]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  1.01201854 263.02389672]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  1.01174971 263.02389682]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  1.01148089 263.02389693]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  1.01121207 263.02389704]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  1.01094325 263.02389715]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  1.01067443 263.02389726]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  1.01040561 263.02389736]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  1.01013679 263.02389747]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  1.00986797 263.02389758]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  1.00959915 263.02389769]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  1.00933033 263.0238978 ]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  1.0090615 263.0238979]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  1.00879268 263.02389801]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  1.00852386 263.02389812]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  1.00825504 263.02389823]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  1.00798622 263.02389834]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  1.0077174  263.02389844]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  1.00744858 263.02389855]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  1.00717976 263.02389866]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  1.00691094 263.02389877]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  1.00664212 263.02389888]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  1.0063733  263.02389898]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  1.00610447 263.02389909]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  1.00583565 263.0238992 ]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  1.00556683 263.02389931]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  1.00529801 263.02389942]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  1.00502919 263.02389952]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  1.00476037 263.02389963]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  1.00449155 263.02389974]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  1.00422273 263.02389985]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  1.00395391 263.02389996]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  1.00368509 263.02390006]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  1.00341626 263.02390017]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  1.00314744 263.02390028]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  1.00287862 263.02390039]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  1.0026098 263.0239005]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  1.00234098 263.0239006 ]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  1.00207216 263.02390071]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  1.00180334 263.02390082]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  1.00153452 263.02390093]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  1.0012657  263.02390104]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  1.00099688 263.02390114]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  1.00072806 263.02390125]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  1.00045923 263.02390136]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  1.00019041 263.02390147]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  0.99992159 263.02390158]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  0.99965277 263.02390168]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  0.99938395 263.02390179]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  0.99911513 263.0239019 ]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  0.99884631 263.02390201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  0.99857749 263.02390212]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  0.99830867 263.02390222]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  0.99803985 263.02390233]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  0.99777102 263.02390244]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  0.9975022  263.02390255]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  0.99723338 263.02390266]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  0.99696456 263.02390276]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  0.99669574 263.02390287]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  0.99642692 263.02390298]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  0.9961581  263.02390309]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  0.99588928 263.0239032 ]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  0.99562046 263.02390331]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  0.99535164 263.02390341]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  0.99508282 263.02390352]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  0.99481399 263.02390363]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  0.99454517 263.02390374]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  0.99427635 263.02390385]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  0.99400753 263.02390395]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  0.99373871 263.02390406]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  0.99346989 263.02390417]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  0.99320107 263.02390428]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  0.99293225 263.02390439]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  0.99266343 263.02390449]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  0.99239461 263.0239046 ]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  0.99212579 263.02390471]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  0.99185697 263.02390482]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  0.99158814 263.02390493]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  0.99131932 263.02390503]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  0.9910505  263.02390514]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  0.99078168 263.02390525]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  0.99051286 263.02390536]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  0.99024404 263.02390547]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  0.98997522 263.02390557]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  0.9897064  263.02390568]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  0.98943758 263.02390579]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  0.98916876 263.0239059 ]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  0.98889994 263.02390601]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  0.98863111 263.02390611]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  0.98836229 263.02390622]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  0.98809347 263.02390633]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  0.98782465 263.02390644]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  0.98755583 263.02390655]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  0.98728701 263.02390665]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  0.98701819 263.02390676]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  0.98674937 263.02390687]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  0.98648055 263.02390698]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  0.98621173 263.02390709]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  0.98594291 263.02390719]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  0.98567409 263.0239073 ]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  0.98540526 263.02390741]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  0.98513644 263.02390752]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  0.98486762 263.02390763]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  0.9845988  263.02390773]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  0.98432998 263.02390784]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  0.98406116 263.02390795]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  0.98379234 263.02390806]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  0.98352352 263.02390817]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  0.9832547  263.02390827]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  0.98298588 263.02390838]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  0.98271706 263.02390849]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  0.98244824 263.0239086 ]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  0.98217941 263.02390871]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  0.98191059 263.02390881]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  0.98164177 263.02390892]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  0.98137295 263.02390903]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  0.98110413 263.02390914]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  0.98083531 263.02390925]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  0.98056649 263.02390935]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  0.98029767 263.02390946]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  0.98002885 263.02390957]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  0.97976003 263.02390968]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  0.97949121 263.02390979]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  0.97922239 263.02390989]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  0.97895356 263.02391   ]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  0.97868474 263.02391011]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  0.97841592 263.02391022]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  0.9781471  263.02391033]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  0.97787828 263.02391044]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  0.97760946 263.02391054]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  0.97734064 263.02391065]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  0.97707182 263.02391076]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  0.976803   263.02391087]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  0.97653418 263.02391098]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  0.97626536 263.02391108]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  0.97599654 263.02391119]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  0.97572771 263.0239113 ]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  0.97545889 263.02391141]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  0.97519007 263.02391152]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  0.97492125 263.02391162]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  0.97465243 263.02391173]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  0.97438361 263.02391184]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  0.97411479 263.02391195]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  0.97384597 263.02391206]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  0.97357715 263.02391216]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  0.97330833 263.02391227]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  0.97303951 263.02391238]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  0.97277069 263.02391249]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  0.97250187 263.0239126 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  0.97223304 263.0239127 ]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  0.97196422 263.02391281]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  0.9716954  263.02391292]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  0.97142658 263.02391303]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  0.97115776 263.02391314]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  0.97088894 263.02391324]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  0.97062012 263.02391335]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  0.9703513  263.02391346]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  0.97008248 263.02391357]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  0.96981366 263.02391368]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  0.96954484 263.02391378]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  0.96927602 263.02391389]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  0.9690072 263.023914 ]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  0.96873837 263.02391411]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  0.96846955 263.02391422]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  0.96820073 263.02391432]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  0.96793191 263.02391443]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  0.96766309 263.02391454]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  0.96739427 263.02391465]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  0.96712545 263.02391476]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  0.96685663 263.02391486]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  0.96658781 263.02391497]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  0.96631899 263.02391508]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  0.96605017 263.02391519]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  0.96578135 263.0239153 ]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  0.96551253 263.0239154 ]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  0.96524371 263.02391551]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  0.96497488 263.02391562]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  0.96470606 263.02391573]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  0.96443724 263.02391584]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  0.96416842 263.02391594]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  0.9638996  263.02391605]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  0.96363078 263.02391616]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  0.96336196 263.02391627]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  0.96309314 263.02391638]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  0.96282432 263.02391648]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  0.9625555  263.02391659]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  0.96228668 263.0239167 ]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  0.96201786 263.02391681]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  0.96174904 263.02391692]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  0.96148022 263.02391702]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  0.96121139 263.02391713]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  0.96094257 263.02391724]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  0.96067375 263.02391735]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  0.96040493 263.02391746]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  0.96013611 263.02391757]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  0.95986729 263.02391767]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  0.95959847 263.02391778]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  0.95932965 263.02391789]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  0.95906083 263.023918  ]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  0.95879201 263.02391811]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  0.95852319 263.02391821]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  0.95825437 263.02391832]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  0.95798555 263.02391843]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  0.95771673 263.02391854]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  0.9574479  263.02391865]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  0.95717908 263.02391875]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  0.95691026 263.02391886]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  0.95664144 263.02391897]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  0.95637262 263.02391908]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  0.9561038  263.02391919]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  0.95583498 263.02391929]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  0.95556616 263.0239194 ]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  0.95529734 263.02391951]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  0.95502852 263.02391962]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  0.9547597  263.02391973]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  0.95449088 263.02391983]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  0.95422206 263.02391994]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  0.95395324 263.02392005]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  0.95368442 263.02392016]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  0.95341559 263.02392027]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  0.95314677 263.02392037]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  0.95287795 263.02392048]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  0.95260913 263.02392059]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  0.95234031 263.0239207 ]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  0.95207149 263.02392081]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  0.95180267 263.02392091]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  0.95153385 263.02392102]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  0.95126503 263.02392113]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  0.95099621 263.02392124]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  0.95072739 263.02392135]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  0.95045857 263.02392145]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  0.95018975 263.02392156]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  0.94992093 263.02392167]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  0.94965211 263.02392178]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  0.94938328 263.02392189]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  0.94911446 263.02392199]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  0.94884564 263.0239221 ]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  0.94857682 263.02392221]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  0.948308   263.02392232]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  0.94803918 263.02392243]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  0.94777036 263.02392253]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  0.94750154 263.02392264]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  0.94723272 263.02392275]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  0.9469639  263.02392286]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  0.94669508 263.02392297]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  0.94642626 263.02392307]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  0.94615744 263.02392318]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  0.94588862 263.02392329]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  0.9456198 263.0239234]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  0.94535098 263.02392351]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  0.94508215 263.02392361]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  0.94481333 263.02392372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  0.94454451 263.02392383]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  0.94427569 263.02392394]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  0.94400687 263.02392405]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  0.94373805 263.02392416]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  0.94346923 263.02392426]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  0.94320041 263.02392437]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  0.94293159 263.02392448]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  0.94266277 263.02392459]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  0.94239395 263.0239247 ]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  0.94212513 263.0239248 ]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  0.94185631 263.02392491]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  0.94158749 263.02392502]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  0.94131867 263.02392513]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  0.94104985 263.02392524]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  0.94078102 263.02392534]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  0.9405122  263.02392545]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  0.94024338 263.02392556]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  0.93997456 263.02392567]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  0.93970574 263.02392578]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  0.93943692 263.02392588]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  0.9391681  263.02392599]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  0.93889928 263.0239261 ]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  0.93863046 263.02392621]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  0.93836164 263.02392632]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  0.93809282 263.02392642]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  0.937824   263.02392653]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  0.93755518 263.02392664]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  0.93728636 263.02392675]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  0.93701754 263.02392686]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  0.93674872 263.02392696]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  0.9364799  263.02392707]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  0.93621108 263.02392718]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  0.93594225 263.02392729]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  0.93567343 263.0239274 ]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  0.93540461 263.0239275 ]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  0.93513579 263.02392761]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  0.93486697 263.02392772]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  0.93459815 263.02392783]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  0.93432933 263.02392794]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  0.93406051 263.02392804]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  0.93379169 263.02392815]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  0.93352287 263.02392826]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  0.93325405 263.02392837]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  0.93298523 263.02392848]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  0.93271641 263.02392858]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  0.93244759 263.02392869]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  0.93217877 263.0239288 ]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  0.93190995 263.02392891]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  0.93164113 263.02392902]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  0.93137231 263.02392912]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  0.93110348 263.02392923]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  0.93083466 263.02392934]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  0.93056584 263.02392945]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  0.93029702 263.02392956]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  0.9300282  263.02392966]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  0.92975938 263.02392977]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  0.92949056 263.02392988]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  0.92922174 263.02392999]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  0.92895292 263.0239301 ]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  0.9286841 263.0239302]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  0.92841528 263.02393031]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  0.92814646 263.02393042]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  0.92787764 263.02393053]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  0.92760882 263.02393064]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  0.92734    263.02393074]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  0.92707118 263.02393085]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  0.92680236 263.02393096]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  0.92653354 263.02393107]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  0.92626472 263.02393118]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  0.92599589 263.02393129]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  0.92572707 263.02393139]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  0.92545825 263.0239315 ]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  0.92518943 263.02393161]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  0.92492061 263.02393172]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  0.92465179 263.02393183]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  0.92438297 263.02393193]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  0.92411415 263.02393204]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  0.92384533 263.02393215]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  0.92357651 263.02393226]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  0.92330769 263.02393237]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  0.92303887 263.02393247]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  0.92277005 263.02393258]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  0.92250123 263.02393269]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  0.92223241 263.0239328 ]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  0.92196359 263.02393291]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  0.92169477 263.02393301]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  0.92142595 263.02393312]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  0.92115713 263.02393323]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  0.92088831 263.02393334]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  0.92061948 263.02393345]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  0.92035066 263.02393355]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  0.92008184 263.02393366]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  0.91981302 263.02393377]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  0.9195442  263.02393388]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  0.91927538 263.02393399]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  0.91900656 263.02393409]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  0.91873774 263.0239342 ]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  0.91846892 263.02393431]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  0.9182001  263.02393442]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  0.91793128 263.02393453]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  0.91766246 263.02393463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  0.91739364 263.02393474]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  0.91712482 263.02393485]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  0.916856   263.02393496]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  0.91658718 263.02393507]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  0.91631836 263.02393517]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  0.91604954 263.02393528]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  0.91578072 263.02393539]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  0.9155119 263.0239355]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  0.91524308 263.02393561]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  0.91497426 263.02393571]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  0.91470543 263.02393582]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  0.91443661 263.02393593]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  0.91416779 263.02393604]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  0.91389897 263.02393615]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  0.91363015 263.02393625]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  0.91336133 263.02393636]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  0.91309251 263.02393647]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  0.91282369 263.02393658]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  0.91255487 263.02393669]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  0.91228605 263.02393679]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  0.91201723 263.0239369 ]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  0.91174841 263.02393701]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  0.91147959 263.02393712]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  0.91121077 263.02393723]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  0.91094195 263.02393733]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  0.91067313 263.02393744]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  0.91040431 263.02393755]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  0.91013549 263.02393766]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  0.90986667 263.02393777]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  0.90959785 263.02393787]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  0.90932903 263.02393798]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  0.90906021 263.02393809]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  0.90879139 263.0239382 ]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  0.90852257 263.02393831]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  0.90825374 263.02393842]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  0.90798492 263.02393852]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  0.9077161  263.02393863]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  0.90744728 263.02393874]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  0.90717846 263.02393885]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  0.90690964 263.02393896]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  0.90664082 263.02393906]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  0.906372   263.02393917]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  0.90610318 263.02393928]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  0.90583436 263.02393939]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  0.90556554 263.0239395 ]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  0.90529672 263.0239396 ]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  0.9050279  263.02393971]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  0.90475908 263.02393982]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  0.90449026 263.02393993]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  0.90422144 263.02394004]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  0.90395262 263.02394014]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  0.9036838  263.02394025]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  0.90341498 263.02394036]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  0.90314616 263.02394047]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  0.90287734 263.02394058]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  0.90260852 263.02394068]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  0.9023397  263.02394079]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  0.90207088 263.0239409 ]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  0.90180206 263.02394101]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  0.90153323 263.02394112]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  0.90126441 263.02394122]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  0.90099559 263.02394133]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  0.90072677 263.02394144]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  0.90045795 263.02394155]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  0.90018913 263.02394166]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  0.89992031 263.02394176]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  0.89965149 263.02394187]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  0.89938267 263.02394198]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  0.89911385 263.02394209]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  0.89884503 263.0239422 ]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  0.89857621 263.0239423 ]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  0.89830739 263.02394241]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  0.89803857 263.02394252]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  0.89776975 263.02394263]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  0.89750093 263.02394274]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  0.89723211 263.02394284]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  0.89696329 263.02394295]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  0.89669447 263.02394306]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  0.89642565 263.02394317]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  0.89615683 263.02394328]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  0.89588801 263.02394338]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  0.89561919 263.02394349]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  0.89535037 263.0239436 ]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  0.89508155 263.02394371]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  0.89481273 263.02394382]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  0.89454391 263.02394392]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  0.89427509 263.02394403]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  0.89400627 263.02394414]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  0.89373744 263.02394425]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  0.89346862 263.02394436]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  0.8931998  263.02394446]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  0.89293098 263.02394457]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  0.89266216 263.02394468]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  0.89239334 263.02394479]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  0.89212452 263.0239449 ]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  0.8918557 263.023945 ]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  0.89158688 263.02394511]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  0.89131806 263.02394522]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  0.89104924 263.02394533]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  0.89078042 263.02394544]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  0.8905116  263.02394555]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  0.89024278 263.02394565]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  0.88997396 263.02394576]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  0.88970514 263.02394587]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  0.88943632 263.02394598]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  0.8891675  263.02394609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  0.88889868 263.02394619]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  0.88862986 263.0239463 ]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  0.88836104 263.02394641]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  0.88809222 263.02394652]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  0.8878234  263.02394663]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  0.88755458 263.02394673]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  0.88728576 263.02394684]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  0.88701694 263.02394695]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  0.88674812 263.02394706]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  0.8864793  263.02394717]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  0.88621048 263.02394727]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  0.88594166 263.02394738]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  0.88567284 263.02394749]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  0.88540402 263.0239476 ]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  0.8851352  263.02394771]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  0.88486638 263.02394781]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  0.88459755 263.02394792]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  0.88432873 263.02394803]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  0.88405991 263.02394814]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  0.88379109 263.02394825]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  0.88352227 263.02394835]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  0.88325345 263.02394846]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  0.88298463 263.02394857]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  0.88271581 263.02394868]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  0.88244699 263.02394879]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  0.88217817 263.02394889]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  0.88190935 263.023949  ]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  0.88164053 263.02394911]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  0.88137171 263.02394922]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  0.88110289 263.02394933]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  0.88083407 263.02394943]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  0.88056525 263.02394954]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  0.88029643 263.02394965]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  0.88002761 263.02394976]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  0.87975879 263.02394987]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  0.87948997 263.02394997]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  0.87922115 263.02395008]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  0.87895233 263.02395019]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  0.87868351 263.0239503 ]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  0.87841469 263.02395041]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  0.87814587 263.02395051]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  0.87787705 263.02395062]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  0.87760823 263.02395073]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  0.87733941 263.02395084]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  0.87707059 263.02395095]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  0.87680177 263.02395105]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  0.87653295 263.02395116]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  0.87626413 263.02395127]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  0.87599531 263.02395138]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  0.87572649 263.02395149]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  0.87545767 263.02395159]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  0.87518885 263.0239517 ]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  0.87492003 263.02395181]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  0.87465121 263.02395192]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  0.87438239 263.02395203]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  0.87411357 263.02395213]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  0.87384475 263.02395224]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  0.87357592 263.02395235]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  0.8733071  263.02395246]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  0.87303828 263.02395257]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  0.87276946 263.02395268]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  0.87250064 263.02395278]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  0.87223182 263.02395289]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  0.871963 263.023953]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  0.87169418 263.02395311]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  0.87142536 263.02395322]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  0.87115654 263.02395332]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  0.87088772 263.02395343]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  0.8706189  263.02395354]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  0.87035008 263.02395365]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  0.87008126 263.02395376]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  0.86981244 263.02395386]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  0.86954362 263.02395397]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  0.8692748  263.02395408]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  0.86900598 263.02395419]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  0.86873716 263.0239543 ]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  0.86846834 263.0239544 ]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  0.86819952 263.02395451]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  0.8679307  263.02395462]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  0.86766188 263.02395473]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  0.86739306 263.02395484]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  0.86712424 263.02395494]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  0.86685542 263.02395505]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  0.8665866  263.02395516]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  0.86631778 263.02395527]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  0.86604896 263.02395538]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  0.86578014 263.02395548]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  0.86551132 263.02395559]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  0.8652425 263.0239557]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  0.86497368 263.02395581]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  0.86470486 263.02395592]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  0.86443604 263.02395602]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  0.86416722 263.02395613]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  0.8638984  263.02395624]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  0.86362958 263.02395635]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  0.86336076 263.02395646]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  0.86309194 263.02395656]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  0.86282312 263.02395667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  0.8625543  263.02395678]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  0.86228548 263.02395689]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  0.86201666 263.023957  ]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  0.86174784 263.0239571 ]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  0.86147902 263.02395721]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  0.8612102  263.02395732]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  0.86094138 263.02395743]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  0.86067256 263.02395754]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  0.86040374 263.02395764]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  0.86013492 263.02395775]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  0.8598661  263.02395786]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  0.85959728 263.02395797]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  0.85932846 263.02395808]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  0.85905964 263.02395818]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  0.85879082 263.02395829]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  0.858522  263.0239584]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  0.85825318 263.02395851]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  0.85798436 263.02395862]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  0.85771554 263.02395872]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [  0.85744672 263.02395883]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [  0.8571779  263.02395894]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [  0.85690907 263.02395905]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [  0.85664025 263.02395916]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [  0.85637143 263.02395927]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [  0.85610261 263.02395937]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [  0.85583379 263.02395948]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [  0.85556497 263.02395959]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [  0.85529615 263.0239597 ]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [  0.85502733 263.02395981]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [  0.85475851 263.02395991]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [  0.85448969 263.02396002]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [  0.85422087 263.02396013]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [  0.85395205 263.02396024]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [  0.85368323 263.02396035]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [  0.85341441 263.02396045]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [  0.85314559 263.02396056]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [  0.85287677 263.02396067]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [  0.85260795 263.02396078]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [  0.85233913 263.02396089]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [  0.85207031 263.02396099]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [  0.85180149 263.0239611 ]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [  0.85153267 263.02396121]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [  0.85126385 263.02396132]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [  0.85099503 263.02396143]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [  0.85072621 263.02396153]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [  0.85045739 263.02396164]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [  0.85018857 263.02396175]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [  0.84991975 263.02396186]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [  0.84965093 263.02396197]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [  0.84938211 263.02396207]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [  0.84911329 263.02396218]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [  0.84884447 263.02396229]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [  0.84857565 263.0239624 ]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [  0.84830683 263.02396251]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [  0.84803801 263.02396261]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [  0.84776919 263.02396272]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [  0.84750037 263.02396283]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [  0.84723155 263.02396294]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [  0.84696273 263.02396305]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [  0.84669391 263.02396315]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [  0.84642509 263.02396326]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [  0.84615627 263.02396337]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [  0.84588745 263.02396348]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [  0.84561863 263.02396359]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [  0.84534981 263.02396369]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [  0.84508099 263.0239638 ]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [  0.84481217 263.02396391]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [  0.84454335 263.02396402]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [  0.84427453 263.02396413]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [  0.84400571 263.02396423]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [  0.84373689 263.02396434]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [  0.84346807 263.02396445]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [  0.84319925 263.02396456]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [  0.84293043 263.02396467]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [  0.84266161 263.02396477]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [  0.84239279 263.02396488]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [  0.84212397 263.02396499]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [  0.84185515 263.0239651 ]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [  0.84158633 263.02396521]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [  0.84131751 263.02396531]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [  0.84104869 263.02396542]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [  0.84077987 263.02396553]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [  0.84051105 263.02396564]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [  0.84024223 263.02396575]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [  0.83997341 263.02396585]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [  0.83970459 263.02396596]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [  0.83943577 263.02396607]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [  0.83916695 263.02396618]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [  0.83889813 263.02396629]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [  0.83862931 263.0239664 ]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [  0.83836049 263.0239665 ]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [  0.83809167 263.02396661]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [  0.83782285 263.02396672]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [  0.83755403 263.02396683]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [  0.83728521 263.02396694]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [  0.83701639 263.02396704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [  0.83674757 263.02396715]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  0.83647875 263.02396726]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(simple_train_feature_matrix, \n",
    "                                                             train_output, inital_weights=initial_weights,\n",
    "                                                             setp_size=step_size, l2_penalty=0, \n",
    "                                                             max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [ 1.01868035 47.95248648]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [ 1.03396516 77.06514406]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [ 1.04714459 95.11630348]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [  1.05901859 106.30883538]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [  1.07008316 113.24870903]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [  1.08064585 117.55174284]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [  1.09089734 120.21981708]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [  1.10095589 121.87414269]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [  1.1108948  122.89989844]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [  1.12075953 123.53591254]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  1.13057826 123.93026925]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  1.14036848 124.17478749]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  1.150141   124.32639911]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  1.15990257 124.42040445]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  1.16965734 124.47869129]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  1.17940789 124.51483105]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  1.18915582 124.53723863]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  1.19890214 124.55113164]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  1.20864745 124.55974522]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  1.21839213 124.56508533]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  1.22813644 124.56839573]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  1.2378805  124.57044762]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  1.24762441 124.57171918]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  1.25736823 124.57250691]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  1.26711199 124.57299462]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  1.27685572 124.57329633]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  1.28659942 124.57348269]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  1.29634311 124.57359754]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  1.3060868  124.57366805]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  1.31583047 124.57371106]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  1.32557415 124.57373703]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  1.33531782 124.57375242]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  1.34506149 124.57376126]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  1.35480516 124.57376604]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  1.36454882 124.5737683 ]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  1.37429249 124.57376899]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  1.38403616 124.57376872]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  1.39377983 124.57376784]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  1.40352349 124.5737666 ]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  1.41326716 124.57376512]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  1.42301083 124.5737635 ]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  1.43275449 124.57376179]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  1.44249816 124.57376002]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  1.45224182 124.57375823]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  1.46198549 124.57375641]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  1.47172915 124.57375457]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  1.48147282 124.57375273]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  1.49121648 124.57375088]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  1.50096015 124.57374903]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  1.51070381 124.57374718]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  1.52044748 124.57374533]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  1.53019114 124.57374348]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  1.5399348  124.57374162]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  1.54967847 124.57373977]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  1.55942213 124.57373792]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  1.56916579 124.57373606]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  1.57890946 124.57373421]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  1.58865312 124.57373235]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  1.59839678 124.5737305 ]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  1.60814044 124.57372864]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  1.61788411 124.57372679]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  1.62762777 124.57372493]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  1.63737143 124.57372308]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  1.64711509 124.57372122]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  1.65685875 124.57371937]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  1.66660241 124.57371752]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  1.67634607 124.57371566]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  1.68608973 124.57371381]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  1.6958334  124.57371195]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  1.70557706 124.5737101 ]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  1.71532072 124.57370824]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  1.72506438 124.57370639]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  1.73480803 124.57370453]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  1.74455169 124.57370268]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  1.75429535 124.57370082]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  1.76403901 124.57369897]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  1.77378267 124.57369712]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  1.78352633 124.57369526]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  1.79326999 124.57369341]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  1.80301365 124.57369155]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  1.8127573 124.5736897]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  1.82250096 124.57368784]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  1.83224462 124.57368599]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  1.84198828 124.57368413]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  1.85173193 124.57368228]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  1.86147559 124.57368042]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  1.87121925 124.57367857]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  1.8809629  124.57367672]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  1.89070656 124.57367486]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  1.90045022 124.57367301]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  1.91019387 124.57367115]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  1.91993753 124.5736693 ]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  1.92968118 124.57366744]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  1.93942484 124.57366559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  1.94916849 124.57366373]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  1.95891215 124.57366188]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  1.9686558  124.57366002]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  1.97839946 124.57365817]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  1.98814311 124.57365632]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  1.99788677 124.57365446]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  2.00763042 124.57365261]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  2.01737407 124.57365075]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  2.02711773 124.5736489 ]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  2.03686138 124.57364704]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  2.04660503 124.57364519]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  2.05634869 124.57364333]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  2.06609234 124.57364148]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  2.07583599 124.57363962]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  2.08557964 124.57363777]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  2.0953233  124.57363592]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  2.10506695 124.57363406]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  2.1148106  124.57363221]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  2.12455425 124.57363035]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  2.1342979 124.5736285]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  2.14404155 124.57362664]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  2.1537852  124.57362479]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  2.16352885 124.57362293]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  2.1732725  124.57362108]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  2.18301615 124.57361922]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  2.1927598  124.57361737]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  2.20250345 124.57361552]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  2.2122471  124.57361366]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  2.22199075 124.57361181]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  2.2317344  124.57360995]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  2.24147805 124.5736081 ]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  2.2512217  124.57360624]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  2.26096535 124.57360439]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  2.270709   124.57360253]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  2.28045265 124.57360068]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  2.29019629 124.57359882]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  2.29993994 124.57359697]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  2.30968359 124.57359512]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  2.31942724 124.57359326]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  2.32917088 124.57359141]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  2.33891453 124.57358955]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  2.34865818 124.5735877 ]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  2.35840182 124.57358584]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  2.36814547 124.57358399]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  2.37788912 124.57358213]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  2.38763276 124.57358028]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  2.39737641 124.57357842]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  2.40712005 124.57357657]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  2.4168637  124.57357472]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  2.42660734 124.57357286]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  2.43635099 124.57357101]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  2.44609463 124.57356915]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  2.45583828 124.5735673 ]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  2.46558192 124.57356544]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  2.47532556 124.57356359]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  2.48506921 124.57356173]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  2.49481285 124.57355988]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  2.5045565  124.57355802]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  2.51430014 124.57355617]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  2.52404378 124.57355432]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  2.53378742 124.57355246]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  2.54353107 124.57355061]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  2.55327471 124.57354875]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  2.56301835 124.5735469 ]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  2.57276199 124.57354504]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  2.58250564 124.57354319]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  2.59224928 124.57354133]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  2.60199292 124.57353948]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  2.61173656 124.57353762]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  2.6214802  124.57353577]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  2.63122384 124.57353392]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  2.64096748 124.57353206]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  2.65071112 124.57353021]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  2.66045476 124.57352835]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  2.6701984 124.5735265]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  2.67994204 124.57352464]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  2.68968568 124.57352279]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  2.69942932 124.57352093]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  2.70917296 124.57351908]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  2.7189166  124.57351722]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  2.72866024 124.57351537]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  2.73840388 124.57351352]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  2.74814751 124.57351166]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  2.75789115 124.57350981]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  2.76763479 124.57350795]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  2.77737843 124.5735061 ]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  2.78712207 124.57350424]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  2.7968657  124.57350239]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  2.80660934 124.57350053]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  2.81635298 124.57349868]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  2.82609661 124.57349682]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  2.83584025 124.57349497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  2.84558389 124.57349312]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  2.85532752 124.57349126]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  2.86507116 124.57348941]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  2.87481479 124.57348755]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  2.88455843 124.5734857 ]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  2.89430207 124.57348384]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  2.9040457  124.57348199]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  2.91378933 124.57348013]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  2.92353297 124.57347828]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  2.9332766  124.57347642]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  2.94302024 124.57347457]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  2.95276387 124.57347272]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  2.96250751 124.57347086]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  2.97225114 124.57346901]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  2.98199477 124.57346715]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  2.99173841 124.5734653 ]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  3.00148204 124.57346344]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  3.01122567 124.57346159]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  3.0209693  124.57345973]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  3.03071294 124.57345788]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  3.04045657 124.57345602]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  3.0502002  124.57345417]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  3.05994383 124.57345232]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  3.06968746 124.57345046]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  3.0794311  124.57344861]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  3.08917473 124.57344675]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  3.09891836 124.5734449 ]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  3.10866199 124.57344304]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  3.11840562 124.57344119]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  3.12814925 124.57343933]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  3.13789288 124.57343748]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  3.14763651 124.57343562]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  3.15738014 124.57343377]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  3.16712377 124.57343192]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  3.1768674  124.57343006]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  3.18661103 124.57342821]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  3.19635466 124.57342635]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  3.20609828 124.5734245 ]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  3.21584191 124.57342264]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  3.22558554 124.57342079]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  3.23532917 124.57341893]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  3.2450728  124.57341708]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  3.25481643 124.57341522]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  3.26456005 124.57341337]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  3.27430368 124.57341152]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  3.28404731 124.57340966]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  3.29379093 124.57340781]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  3.30353456 124.57340595]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  3.31327819 124.5734041 ]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  3.32302181 124.57340224]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  3.33276544 124.57340039]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  3.34250906 124.57339853]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  3.35225269 124.57339668]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  3.36199632 124.57339482]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  3.37173994 124.57339297]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  3.38148357 124.57339112]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  3.39122719 124.57338926]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  3.40097082 124.57338741]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  3.41071444 124.57338555]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  3.42045806 124.5733837 ]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  3.43020169 124.57338184]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  3.43994531 124.57337999]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  3.44968894 124.57337813]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  3.45943256 124.57337628]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  3.46917618 124.57337442]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  3.4789198  124.57337257]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  3.48866343 124.57337072]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  3.49840705 124.57336886]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  3.50815067 124.57336701]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  3.51789429 124.57336515]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  3.52763792 124.5733633 ]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  3.53738154 124.57336144]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  3.54712516 124.57335959]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  3.55686878 124.57335773]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  3.5666124  124.57335588]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  3.57635602 124.57335402]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  3.58609964 124.57335217]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  3.59584326 124.57335032]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  3.60558688 124.57334846]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  3.6153305  124.57334661]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  3.62507412 124.57334475]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  3.63481774 124.5733429 ]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  3.64456136 124.57334104]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  3.65430498 124.57333919]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  3.6640486  124.57333733]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  3.67379222 124.57333548]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  3.68353584 124.57333362]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  3.69327946 124.57333177]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  3.70302308 124.57332992]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  3.71276669 124.57332806]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  3.72251031 124.57332621]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  3.73225393 124.57332435]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  3.74199755 124.5733225 ]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  3.75174116 124.57332064]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  3.76148478 124.57331879]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  3.7712284  124.57331693]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  3.78097202 124.57331508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  3.79071563 124.57331322]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  3.80045925 124.57331137]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  3.81020286 124.57330952]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  3.81994648 124.57330766]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  3.8296901  124.57330581]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  3.83943371 124.57330395]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  3.84917733 124.5733021 ]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  3.85892094 124.57330024]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  3.86866456 124.57329839]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  3.87840817 124.57329653]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  3.88815178 124.57329468]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  3.8978954  124.57329282]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  3.90763901 124.57329097]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  3.91738263 124.57328912]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  3.92712624 124.57328726]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  3.93686985 124.57328541]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  3.94661347 124.57328355]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  3.95635708 124.5732817 ]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  3.96610069 124.57327984]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  3.9758443  124.57327799]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  3.98558792 124.57327613]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  3.99533153 124.57327428]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  4.00507514 124.57327242]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  4.01481875 124.57327057]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  4.02456236 124.57326872]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  4.03430598 124.57326686]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  4.04404959 124.57326501]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  4.0537932  124.57326315]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  4.06353681 124.5732613 ]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  4.07328042 124.57325944]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  4.08302403 124.57325759]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  4.09276764 124.57325573]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  4.10251125 124.57325388]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  4.11225486 124.57325202]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  4.12199847 124.57325017]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  4.13174208 124.57324832]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  4.14148569 124.57324646]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  4.15122929 124.57324461]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  4.1609729  124.57324275]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  4.17071651 124.5732409 ]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  4.18046012 124.57323904]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  4.19020373 124.57323719]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  4.19994734 124.57323533]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  4.20969094 124.57323348]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  4.21943455 124.57323162]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  4.22917816 124.57322977]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  4.23892177 124.57322792]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  4.24866537 124.57322606]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  4.25840898 124.57322421]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  4.26815259 124.57322235]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  4.27789619 124.5732205 ]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  4.2876398  124.57321864]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  4.2973834  124.57321679]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  4.30712701 124.57321493]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  4.31687061 124.57321308]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  4.32661422 124.57321122]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  4.33635782 124.57320937]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  4.34610143 124.57320752]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  4.35584503 124.57320566]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  4.36558864 124.57320381]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  4.37533224 124.57320195]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  4.38507585 124.5732001 ]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  4.39481945 124.57319824]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  4.40456305 124.57319639]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  4.41430666 124.57319453]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  4.42405026 124.57319268]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  4.43379386 124.57319082]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  4.44353747 124.57318897]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  4.45328107 124.57318712]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  4.46302467 124.57318526]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  4.47276827 124.57318341]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  4.48251187 124.57318155]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  4.49225548 124.5731797 ]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  4.50199908 124.57317784]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  4.51174268 124.57317599]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  4.52148628 124.57317413]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  4.53122988 124.57317228]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  4.54097348 124.57317042]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  4.55071708 124.57316857]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  4.56046068 124.57316672]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  4.57020428 124.57316486]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  4.57994788 124.57316301]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  4.58969148 124.57316115]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  4.59943508 124.5731593 ]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  4.60917868 124.57315744]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  4.61892228 124.57315559]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  4.62866588 124.57315373]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  4.63840948 124.57315188]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  4.64815307 124.57315002]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  4.65789667 124.57314817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  4.66764027 124.57314632]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  4.67738387 124.57314446]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  4.68712747 124.57314261]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  4.69687106 124.57314075]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  4.70661466 124.5731389 ]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  4.71635826 124.57313704]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  4.72610185 124.57313519]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  4.73584545 124.57313333]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  4.74558905 124.57313148]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  4.75533264 124.57312962]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  4.76507624 124.57312777]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  4.77481984 124.57312592]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  4.78456343 124.57312406]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  4.79430703 124.57312221]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  4.80405062 124.57312035]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  4.81379422 124.5731185 ]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  4.82353781 124.57311664]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  4.83328141 124.57311479]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  4.843025   124.57311293]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  4.85276859 124.57311108]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  4.86251219 124.57310922]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  4.87225578 124.57310737]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  4.88199937 124.57310552]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  4.89174297 124.57310366]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  4.90148656 124.57310181]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  4.91123015 124.57309995]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  4.92097375 124.5730981 ]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  4.93071734 124.57309624]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  4.94046093 124.57309439]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  4.95020452 124.57309253]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  4.95994812 124.57309068]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  4.96969171 124.57308882]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  4.9794353  124.57308697]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  4.98917889 124.57308512]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  4.99892248 124.57308326]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  5.00866607 124.57308141]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  5.01840966 124.57307955]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  5.02815325 124.5730777 ]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  5.03789684 124.57307584]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  5.04764043 124.57307399]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  5.05738402 124.57307213]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  5.06712761 124.57307028]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  5.0768712  124.57306842]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  5.08661479 124.57306657]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  5.09635838 124.57306472]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  5.10610197 124.57306286]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  5.11584556 124.57306101]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  5.12558915 124.57305915]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  5.13533273 124.5730573 ]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  5.14507632 124.57305544]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  5.15481991 124.57305359]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  5.1645635  124.57305173]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  5.17430708 124.57304988]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  5.18405067 124.57304802]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  5.19379426 124.57304617]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  5.20353785 124.57304432]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  5.21328143 124.57304246]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  5.22302502 124.57304061]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  5.2327686  124.57303875]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  5.24251219 124.5730369 ]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  5.25225578 124.57303504]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  5.26199936 124.57303319]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  5.27174295 124.57303133]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  5.28148653 124.57302948]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  5.29123012 124.57302762]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  5.3009737  124.57302577]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  5.31071729 124.57302392]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  5.32046087 124.57302206]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  5.33020445 124.57302021]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  5.33994804 124.57301835]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  5.34969162 124.5730165 ]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  5.35943521 124.57301464]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  5.36917879 124.57301279]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  5.37892237 124.57301093]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  5.38866595 124.57300908]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  5.39840954 124.57300722]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  5.40815312 124.57300537]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  5.4178967  124.57300352]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  5.42764028 124.57300166]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  5.43738387 124.57299981]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  5.44712745 124.57299795]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  5.45687103 124.5729961 ]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  5.46661461 124.57299424]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  5.47635819 124.57299239]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  5.48610177 124.57299053]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  5.49584535 124.57298868]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  5.50558893 124.57298682]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  5.51533251 124.57298497]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  5.52507609 124.57298312]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  5.53481967 124.57298126]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  5.54456325 124.57297941]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  5.55430683 124.57297755]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  5.56405041 124.5729757 ]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  5.57379399 124.57297384]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  5.58353757 124.57297199]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  5.59328115 124.57297013]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  5.60302473 124.57296828]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  5.6127683  124.57296642]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  5.62251188 124.57296457]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  5.63225546 124.57296272]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  5.64199904 124.57296086]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  5.65174261 124.57295901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  5.66148619 124.57295715]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  5.67122977 124.5729553 ]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  5.68097334 124.57295344]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  5.69071692 124.57295159]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  5.7004605  124.57294973]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  5.71020407 124.57294788]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  5.71994765 124.57294603]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  5.72969123 124.57294417]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  5.7394348  124.57294232]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  5.74917838 124.57294046]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  5.75892195 124.57293861]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  5.76866553 124.57293675]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  5.7784091 124.5729349]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  5.78815267 124.57293304]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  5.79789625 124.57293119]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  5.80763982 124.57292933]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  5.8173834  124.57292748]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  5.82712697 124.57292563]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  5.83687054 124.57292377]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  5.84661412 124.57292192]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  5.85635769 124.57292006]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  5.86610126 124.57291821]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  5.87584484 124.57291635]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  5.88558841 124.5729145 ]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  5.89533198 124.57291264]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  5.90507555 124.57291079]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  5.91481912 124.57290893]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  5.9245627  124.57290708]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  5.93430627 124.57290523]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  5.94404984 124.57290337]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  5.95379341 124.57290152]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  5.96353698 124.57289966]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  5.97328055 124.57289781]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  5.98302412 124.57289595]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  5.99276769 124.5728941 ]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  6.00251126 124.57289224]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  6.01225483 124.57289039]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  6.0219984  124.57288853]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  6.03174197 124.57288668]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  6.04148554 124.57288483]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  6.05122911 124.57288297]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  6.06097268 124.57288112]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  6.07071625 124.57287926]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  6.08045981 124.57287741]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  6.09020338 124.57287555]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  6.09994695 124.5728737 ]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  6.10969052 124.57287184]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  6.11943408 124.57286999]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  6.12917765 124.57286813]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  6.13892122 124.57286628]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  6.14866479 124.57286443]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  6.15840835 124.57286257]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  6.16815192 124.57286072]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  6.17789549 124.57285886]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  6.18763905 124.57285701]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  6.19738262 124.57285515]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  6.20712618 124.5728533 ]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  6.21686975 124.57285144]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  6.22661331 124.57284959]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  6.23635688 124.57284773]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  6.24610044 124.57284588]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  6.25584401 124.57284403]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  6.26558757 124.57284217]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  6.27533114 124.57284032]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  6.2850747  124.57283846]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  6.29481826 124.57283661]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  6.30456183 124.57283475]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  6.31430539 124.5728329 ]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  6.32404896 124.57283104]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  6.33379252 124.57282919]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  6.34353608 124.57282733]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  6.35327964 124.57282548]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  6.36302321 124.57282363]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  6.37276677 124.57282177]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  6.38251033 124.57281992]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  6.39225389 124.57281806]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  6.40199745 124.57281621]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  6.41174101 124.57281435]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  6.42148458 124.5728125 ]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  6.43122814 124.57281064]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  6.4409717  124.57280879]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  6.45071526 124.57280693]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  6.46045882 124.57280508]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  6.47020238 124.57280323]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  6.47994594 124.57280137]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  6.4896895  124.57279952]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  6.49943306 124.57279766]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  6.50917662 124.57279581]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  6.51892018 124.57279395]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  6.52866374 124.5727921 ]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  6.53840729 124.57279024]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  6.54815085 124.57278839]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  6.55789441 124.57278653]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  6.56763797 124.57278468]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  6.57738153 124.57278283]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  6.58712508 124.57278097]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  6.59686864 124.57277912]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  6.6066122  124.57277726]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  6.61635576 124.57277541]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  6.62609931 124.57277355]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  6.63584287 124.5727717 ]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  6.64558643 124.57276984]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  6.65532998 124.57276799]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  6.66507354 124.57276614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  6.67481709 124.57276428]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  6.68456065 124.57276243]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  6.69430421 124.57276057]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  6.70404776 124.57275872]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  6.71379132 124.57275686]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  6.72353487 124.57275501]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  6.73327842 124.57275315]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  6.74302198 124.5727513 ]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  6.75276553 124.57274944]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  6.76250909 124.57274759]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  6.77225264 124.57274574]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  6.78199619 124.57274388]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  6.79173975 124.57274203]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  6.8014833  124.57274017]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  6.81122685 124.57273832]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  6.82097041 124.57273646]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  6.83071396 124.57273461]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  6.84045751 124.57273275]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  6.85020106 124.5727309 ]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  6.85994462 124.57272904]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  6.86968817 124.57272719]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  6.87943172 124.57272534]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  6.88917527 124.57272348]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  6.89891882 124.57272163]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  6.90866237 124.57271977]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  6.91840592 124.57271792]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  6.92814947 124.57271606]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  6.93789302 124.57271421]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  6.94763657 124.57271235]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  6.95738012 124.5727105 ]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  6.96712367 124.57270864]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  6.97686722 124.57270679]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  6.98661077 124.57270494]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  6.99635432 124.57270308]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  7.00609787 124.57270123]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  7.01584142 124.57269937]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  7.02558497 124.57269752]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  7.03532852 124.57269566]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  7.04507206 124.57269381]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  7.05481561 124.57269195]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  7.06455916 124.5726901 ]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  7.07430271 124.57268824]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  7.08404625 124.57268639]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  7.0937898  124.57268454]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  7.10353335 124.57268268]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  7.11327689 124.57268083]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  7.12302044 124.57267897]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  7.13276399 124.57267712]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  7.14250753 124.57267526]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  7.15225108 124.57267341]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  7.16199462 124.57267155]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  7.17173817 124.5726697 ]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  7.18148172 124.57266784]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  7.19122526 124.57266599]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  7.20096881 124.57266414]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  7.21071235 124.57266228]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  7.22045589 124.57266043]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  7.23019944 124.57265857]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  7.23994298 124.57265672]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  7.24968653 124.57265486]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  7.25943007 124.57265301]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  7.26917361 124.57265115]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  7.27891716 124.5726493 ]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  7.2886607  124.57264745]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  7.29840424 124.57264559]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  7.30814778 124.57264374]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  7.31789133 124.57264188]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  7.32763487 124.57264003]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  7.33737841 124.57263817]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  7.34712195 124.57263632]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  7.35686549 124.57263446]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  7.36660904 124.57263261]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  7.37635258 124.57263075]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  7.38609612 124.5726289 ]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  7.39583966 124.57262705]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  7.4055832  124.57262519]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  7.41532674 124.57262334]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  7.42507028 124.57262148]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  7.43481382 124.57261963]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  7.44455736 124.57261777]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  7.4543009  124.57261592]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  7.46404444 124.57261406]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  7.47378798 124.57261221]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  7.48353152 124.57261035]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  7.49327505 124.5726085 ]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  7.50301859 124.57260665]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  7.51276213 124.57260479]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  7.52250567 124.57260294]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  7.53224921 124.57260108]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  7.54199274 124.57259923]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  7.55173628 124.57259737]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  7.56147982 124.57259552]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  7.57122336 124.57259366]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  7.58096689 124.57259181]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  7.59071043 124.57258995]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  7.60045397 124.5725881 ]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  7.6101975  124.57258625]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  7.61994104 124.57258439]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  7.62968457 124.57258254]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  7.63942811 124.57258068]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  7.64917164 124.57257883]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  7.65891518 124.57257697]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  7.66865871 124.57257512]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  7.67840225 124.57257326]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  7.68814578 124.57257141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  7.69788932 124.57256955]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  7.70763285 124.5725677 ]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  7.71737639 124.57256585]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  7.72711992 124.57256399]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  7.73686345 124.57256214]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  7.74660699 124.57256028]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  7.75635052 124.57255843]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  7.76609405 124.57255657]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  7.77583759 124.57255472]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  7.78558112 124.57255286]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  7.79532465 124.57255101]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  7.80506818 124.57254915]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  7.81481171 124.5725473 ]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  7.82455525 124.57254545]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  7.83429878 124.57254359]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  7.84404231 124.57254174]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  7.85378584 124.57253988]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  7.86352937 124.57253803]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  7.8732729  124.57253617]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  7.88301643 124.57253432]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  7.89275996 124.57253246]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  7.90250349 124.57253061]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  7.91224702 124.57252876]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  7.92199055 124.5725269 ]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  7.93173408 124.57252505]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  7.94147761 124.57252319]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  7.95122114 124.57252134]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  7.96096467 124.57251948]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  7.9707082  124.57251763]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  7.98045173 124.57251577]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  7.99019525 124.57251392]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  7.99993878 124.57251206]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  8.00968231 124.57251021]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  8.01942584 124.57250836]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  8.02916936 124.5725065 ]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  8.03891289 124.57250465]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  8.04865642 124.57250279]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  8.05839995 124.57250094]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  8.06814347 124.57249908]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  8.077887   124.57249723]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  8.08763052 124.57249537]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  8.09737405 124.57249352]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  8.10711758 124.57249166]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  8.1168611  124.57248981]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  8.12660463 124.57248796]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  8.13634815 124.5724861 ]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  8.14609168 124.57248425]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  8.1558352  124.57248239]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  8.16557873 124.57248054]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  8.17532225 124.57247868]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  8.18506577 124.57247683]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  8.1948093  124.57247497]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  8.20455282 124.57247312]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  8.21429635 124.57247126]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  8.22403987 124.57246941]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  8.23378339 124.57246756]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  8.24352691 124.5724657 ]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  8.25327044 124.57246385]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  8.26301396 124.57246199]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  8.27275748 124.57246014]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  8.282501   124.57245828]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  8.29224453 124.57245643]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  8.30198805 124.57245457]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  8.31173157 124.57245272]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  8.32147509 124.57245087]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  8.33121861 124.57244901]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  8.34096213 124.57244716]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  8.35070565 124.5724453 ]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  8.36044917 124.57244345]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  8.37019269 124.57244159]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  8.37993621 124.57243974]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  8.38967973 124.57243788]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  8.39942325 124.57243603]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  8.40916677 124.57243417]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  8.41891029 124.57243232]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  8.42865381 124.57243047]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  8.43839733 124.57242861]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  8.44814085 124.57242676]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  8.45788437 124.5724249 ]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  8.46762788 124.57242305]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  8.4773714  124.57242119]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  8.48711492 124.57241934]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  8.49685844 124.57241748]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  8.50660196 124.57241563]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  8.51634547 124.57241377]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  8.52608899 124.57241192]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  8.53583251 124.57241007]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  8.54557602 124.57240821]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  8.55531954 124.57240636]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  8.56506306 124.5724045 ]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  8.57480657 124.57240265]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  8.58455009 124.57240079]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  8.5942936  124.57239894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  8.60403712 124.57239708]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  8.61378063 124.57239523]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  8.62352415 124.57239337]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  8.63326766 124.57239152]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  8.64301118 124.57238967]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  8.65275469 124.57238781]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  8.66249821 124.57238596]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  8.67224172 124.5723841 ]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  8.68198523 124.57238225]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  8.69172875 124.57238039]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  8.70147226 124.57237854]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  8.71121577 124.57237668]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  8.72095929 124.57237483]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  8.7307028  124.57237297]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  8.74044631 124.57237112]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  8.75018982 124.57236927]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  8.75993334 124.57236741]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  8.76967685 124.57236556]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  8.77942036 124.5723637 ]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  8.78916387 124.57236185]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  8.79890738 124.57235999]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  8.80865089 124.57235814]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  8.8183944  124.57235628]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  8.82813791 124.57235443]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  8.83788142 124.57235258]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  8.84762493 124.57235072]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  8.85736844 124.57234887]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  8.86711195 124.57234701]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  8.87685546 124.57234516]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  8.88659897 124.5723433 ]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  8.89634248 124.57234145]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  8.90608599 124.57233959]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  8.9158295  124.57233774]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  8.92557301 124.57233588]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  8.93531652 124.57233403]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  8.94506003 124.57233218]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  8.95480353 124.57233032]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  8.96454704 124.57232847]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  8.97429055 124.57232661]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  8.98403406 124.57232476]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  8.99377756 124.5723229 ]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  9.00352107 124.57232105]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  9.01326458 124.57231919]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  9.02300808 124.57231734]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  9.03275159 124.57231548]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  9.0424951  124.57231363]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  9.0522386  124.57231178]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  9.06198211 124.57230992]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  9.07172561 124.57230807]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  9.08146912 124.57230621]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  9.09121262 124.57230436]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  9.10095613 124.5723025 ]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  9.11069963 124.57230065]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  9.12044314 124.57229879]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  9.13018664 124.57229694]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  9.13993015 124.57229508]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  9.14967365 124.57229323]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  9.15941715 124.57229138]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  9.16916066 124.57228952]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  9.17890416 124.57228767]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  9.18864766 124.57228581]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  9.19839117 124.57228396]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  9.20813467 124.5722821 ]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  9.21787817 124.57228025]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  9.22762167 124.57227839]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  9.23736517 124.57227654]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  9.24710868 124.57227469]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  9.25685218 124.57227283]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  9.26659568 124.57227098]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  9.27633918 124.57226912]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  9.28608268 124.57226727]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  9.29582618 124.57226541]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  9.30556968 124.57226356]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  9.31531318 124.5722617 ]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  9.32505668 124.57225985]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  9.33480018 124.57225799]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  9.34454368 124.57225614]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  9.35428718 124.57225429]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  9.36403068 124.57225243]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  9.37377418 124.57225058]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  9.38351768 124.57224872]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  9.39326118 124.57224687]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  9.40300468 124.57224501]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  9.41274818 124.57224316]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  9.42249167 124.5722413 ]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  9.43223517 124.57223945]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  9.44197867 124.57223759]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  9.45172217 124.57223574]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  9.46146566 124.57223389]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  9.47120916 124.57223203]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  9.48095266 124.57223018]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  9.49069616 124.57222832]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  9.50043965 124.57222647]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  9.51018315 124.57222461]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  9.51992664 124.57222276]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  9.52967014 124.5722209 ]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  9.53941364 124.57221905]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  9.54915713 124.57221719]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  9.55890063 124.57221534]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  9.56864412 124.57221349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  9.57838762 124.57221163]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  9.58813111 124.57220978]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  9.59787461 124.57220792]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  9.6076181  124.57220607]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  9.61736159 124.57220421]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  9.62710509 124.57220236]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  9.63684858 124.5722005 ]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  9.64659207 124.57219865]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  9.65633557 124.5721968 ]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  9.66607906 124.57219494]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  9.67582255 124.57219309]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  9.68556605 124.57219123]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  9.69530954 124.57218938]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  9.70505303 124.57218752]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  9.71479652 124.57218567]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  9.72454001 124.57218381]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  9.73428351 124.57218196]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  9.744027  124.5721801]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  9.75377049 124.57217825]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  9.76351398 124.5721764 ]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  9.77325747 124.57217454]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  9.78300096 124.57217269]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  9.79274445 124.57217083]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  9.80248794 124.57216898]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  9.81223143 124.57216712]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  9.82197492 124.57216527]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  9.83171841 124.57216341]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  9.8414619  124.57216156]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  9.85120539 124.5721597 ]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  9.86094888 124.57215785]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  9.87069237 124.572156  ]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  9.88043586 124.57215414]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  9.89017934 124.57215229]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  9.89992283 124.57215043]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  9.90966632 124.57214858]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  9.91940981 124.57214672]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  9.92915329 124.57214487]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  9.93889678 124.57214301]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  9.94864027 124.57214116]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  9.95838376 124.57213931]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  9.96812724 124.57213745]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  9.97787073 124.5721356 ]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  9.98761422 124.57213374]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  9.9973577  124.57213189]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ 10.00710119 124.57213003]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ 10.01684467 124.57212818]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ 10.02658816 124.57212632]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ 10.03633164 124.57212447]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ 10.04607513 124.57212261]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ 10.05581861 124.57212076]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ 10.0655621  124.57211891]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ 10.07530558 124.57211705]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ 10.08504907 124.5721152 ]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ 10.09479255 124.57211334]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ 10.10453603 124.57211149]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ 10.11427952 124.57210963]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ 10.124023   124.57210778]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ 10.13376648 124.57210592]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ 10.14350997 124.57210407]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ 10.15325345 124.57210221]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ 10.16299693 124.57210036]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ 10.17274042 124.57209851]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ 10.1824839  124.57209665]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ 10.19222738 124.5720948 ]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ 10.20197086 124.57209294]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ 10.21171434 124.57209109]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ 10.22145782 124.57208923]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ 10.23120131 124.57208738]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ 10.24094479 124.57208552]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ 10.25068827 124.57208367]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ 10.26043175 124.57208181]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ 10.27017523 124.57207996]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ 10.27991871 124.57207811]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ 10.28966219 124.57207625]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ 10.29940567 124.5720744 ]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ 10.30914915 124.57207254]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ 10.31889263 124.57207069]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ 10.32863611 124.57206883]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ 10.33837958 124.57206698]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ 10.34812306 124.57206512]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ 10.35786654 124.57206327]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ 10.36761002 124.57206142]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ 10.3773535  124.57205956]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ 10.38709698 124.57205771]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ 10.39684045 124.57205585]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ 10.40658393 124.572054  ]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ 10.41632741 124.57205214]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ 10.42607088 124.57205029]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ 10.43581436 124.57204843]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ 10.44555784 124.57204658]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ 10.45530131 124.57204472]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ 10.46504479 124.57204287]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ 10.47478827 124.57204102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ 10.48453174 124.57203916]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ 10.49427522 124.57203731]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ 10.50401869 124.57203545]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ 10.51376217 124.5720336 ]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ 10.52350564 124.57203174]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ 10.53324912 124.57202989]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ 10.54299259 124.57202803]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ 10.55273607 124.57202618]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ 10.56247954 124.57202432]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ 10.57222302 124.57202247]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ 10.58196649 124.57202062]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ 10.59170996 124.57201876]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ 10.60145344 124.57201691]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ 10.61119691 124.57201505]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ 10.62094038 124.5720132 ]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ 10.63068386 124.57201134]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ 10.64042733 124.57200949]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ 10.6501708  124.57200763]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ 10.65991427 124.57200578]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ 10.66965774 124.57200393]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ 10.67940122 124.57200207]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ 10.68914469 124.57200022]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ 10.69888816 124.57199836]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ 10.70863163 124.57199651]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ 10.7183751  124.57199465]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ 10.72811857 124.5719928 ]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ 10.73786204 124.57199094]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ 10.74760551 124.57198909]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ 10.75734898 124.57198723]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ 10.76709245 124.57198538]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_high_penalty = ridge_regression_gradient_descent(simple_train_feature_matrix, \n",
    "                                                             train_output, inital_weights=initial_weights,\n",
    "                                                             setp_size=step_size, l2_penalty=1e11, \n",
    "                                                             max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f758724a400>,\n",
       " <matplotlib.lines.Line2D at 0x7f758724a520>,\n",
       " <matplotlib.lines.Line2D at 0x7f758724a2b0>,\n",
       " <matplotlib.lines.Line2D at 0x7f758724a430>,\n",
       " <matplotlib.lines.Line2D at 0x7f758724a4f0>,\n",
       " <matplotlib.lines.Line2D at 0x7f758724a760>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(simple_train_feature_matrix,train_output,'k.',\n",
    "        simple_train_feature_matrix,predict_output(simple_train_feature_matrix, simple_weights_0_penalty),'b-',\n",
    "        simple_train_feature_matrix,predict_output(simple_train_feature_matrix, simple_weights_high_penalty),'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claculate error on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1772834159655075.0 275723670696047.84 694640848645739.6\n"
     ]
    }
   ],
   "source": [
    "inital_weights = np.ones(2)\n",
    "prediction_all_zeros = predict_output(simple_test_feature_matrix, inital_weights)\n",
    "prediction_no_penalty = predict_output(simple_test_feature_matrix, simple_weights_0_penalty)\n",
    "prediction_high_penalty = predict_output(simple_test_feature_matrix, simple_weights_high_penalty)\n",
    "\n",
    "RSS_all_zeros = sum((test_output - prediction_all_zeros)**2)\n",
    "RSS_no_penalty = sum((test_output - prediction_no_penalty)**2)\n",
    "RSS_high_penalty = sum((test_output - prediction_high_penalty)**2)\n",
    "print(RSS_all_zeros, RSS_no_penalty, RSS_high_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Running a multiple regression with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15']\n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.ones(3)\n",
    "step_size = 1e-12\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [ 1.0186113  47.99222672 43.07728289]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [ 1.03091879 79.78467516 71.16405163]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [  1.03898774 101.35473687  89.84399183]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [  1.04420693 116.04951686 102.19944276]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [  1.0475103  126.11981714 110.30314799]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [  1.04952584 133.0792194  115.54893803]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [  1.05067587 137.94555388 118.87410138]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [  1.05124437 141.40327765 120.90892019]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [  1.05142232 143.91266745 122.07722708]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [  1.05133812 145.78324765 122.66402556]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  1.05107812 147.22316759 122.86096218]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  1.0507004  148.37240588 122.7969046 ]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  1.05024398 149.32509898 122.55850428]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  1.04973513 150.1445563  122.20402396]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  1.04919149 150.87335721 121.77263617]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  1.04862494 151.54014098 121.29067602]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  1.04804344 152.16417259 120.77584606]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  1.04745236 152.75841322 120.24004378]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  1.04685531 153.33158492 119.69126309]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  1.04625469 153.88955906 119.13487287]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  1.04565215 154.43629003 118.57447687]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  1.04504876 154.97444305 118.01249188]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  1.04444525 155.50581642 117.45053653]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  1.04384212 156.03162541 116.88969277]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  1.04323969 156.55269323 116.33068171]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  1.04263818 157.06957936 115.77398181]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  1.04203772 157.58266593 115.21990842]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  1.04143842 158.09221577 114.66866726]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  1.04084033 158.59841143 114.12039033]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  1.04024349 159.10138147 113.5751601 ]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  1.03964793 159.6012181  113.0330258 ]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  1.03905365 160.09798903 112.49401431]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  1.03846067 160.59174548 111.95813753]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  1.03786897 161.08252757 111.42539734]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  1.03727857 161.57036787 110.89578888]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  1.03668946 162.05529388 110.36930285]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  1.03610164 162.53732965 109.84592693]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  1.03551509 163.01649686 109.32564686]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  1.03492982 163.49281558 108.8084471 ]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  1.03434581 163.96630476 108.29431126]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  1.03376306 164.43698256 107.78322245]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  1.03318156 164.9048666  107.27516344]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  1.0326013  165.36997407 106.77011684]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  1.03202229 165.83232186 106.26806517]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  1.0314445  166.29192661 105.76899092]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  1.03086793 166.7488048  105.27287661]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  1.03029258 167.20297272 104.7797048 ]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  1.02971844 167.65444653 104.28945811]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  1.0291455  168.10324227 103.80211925]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  1.02857376 168.54937586 103.317671  ]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  1.0280032  168.99286313 102.83609624]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  1.02743382 169.43371979 102.35737795]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  1.02686562 169.87196144 101.88149919]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  1.02629858 170.30760362 101.40844313]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  1.0257327  170.74066174 100.93819302]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  1.02516797 171.17115114 100.47073223]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  1.02460438 171.59908706 100.00604422]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  1.02404194 172.02448464  99.54411253]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  1.02348063 172.44735893  99.08492083]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  1.02292044 172.86772492  98.62845285]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  1.02236137 173.28559747  98.17469245]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  1.02180341 173.70099138  97.72362357]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  1.02124656 174.11392135  97.27523023]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  1.02069081 174.524402    96.82949657]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  1.02013615 174.93244785  96.38640681]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  1.01958257 175.33807334  95.94594528]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  1.01903008 175.74129284  95.50809637]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  1.01847866 176.14212061  95.0728446 ]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  1.0179283  176.54057084  94.64017456]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  1.017379   176.93665764  94.21007093]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  1.01683076 177.33039501  93.7825185 ]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  1.01628357 177.72179691  93.35750213]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  1.01573741 178.11087717  92.93500677]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  1.0151923  178.49764958  92.51501748]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  1.01464821 178.88212781  92.09751938]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  1.01410514 179.26432549  91.68249771]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  1.01356309 179.64425613  91.26993777]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  1.01302205 180.02193319  90.85982496]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  1.01248202 180.39737002  90.45214476]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  1.01194299 180.77057993  90.04688276]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  1.01140495 181.14157611  89.64402459]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  1.01086789 181.5103717   89.24355601]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  1.01033182 181.87697975  88.84546284]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  1.00979672 182.24141324  88.44973099]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  1.00926259 182.60368507  88.05634645]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  1.00872943 182.96380805  87.6652953 ]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  1.00819722 183.32179494  87.27656371]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  1.00766597 183.6776584   86.8901379 ]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  1.00713567 184.03141103  86.50600421]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  1.0066063  184.38306535  86.12414904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  1.00607787 184.73263381  85.74455887]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  1.00555037 185.08012878  85.36722026]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  1.0050238  185.42556255  84.99211988]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  1.00449814 185.76894736  84.61924442]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  1.0039734  186.11029535  84.24858071]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  1.00344957 186.44961862  83.88011562]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  1.00292664 186.78692916  83.5138361 ]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  1.0024046  187.12223892  83.1497292 ]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  1.00188346 187.45555977  82.78778203]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  1.00136321 187.7869035   82.42798178]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  1.00084384 188.11628184  82.07031571]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  1.00032534 188.44370645  81.71477116]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  0.99980771 188.76918891  81.36133556]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  0.99929096 189.09274075  81.00999638]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  0.99877506 189.41437342  80.6607412 ]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  0.99826001 189.73409831  80.31355765]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  0.99774582 190.05192672  79.96843345]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  0.99723248 190.3678699   79.62535638]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  0.99671997 190.68193905  79.2843143 ]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  0.9962083  190.99414527  78.94529513]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  0.99569746 191.30449962  78.60828688]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  0.99518745 191.61301308  78.27327763]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  0.99467826 191.91969657  77.9402555 ]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  0.99416988 192.22456094  77.60920873]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  0.99366232 192.52761699  77.28012558]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  0.99315556 192.82887543  76.95299441]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  0.9926496  193.12834694  76.62780365]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  0.99214444 193.42604212  76.30454178]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  0.99164007 193.72197149  75.98319737]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  0.99113649 194.01614553  75.66375903]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  0.99063369 194.30857466  75.34621547]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  0.99013167 194.59926923  75.03055544]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  0.98963043 194.88823952  74.71676777]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  0.98912995 195.17549576  74.40484136]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  0.98863024 195.46104812  74.09476516]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  0.98813128 195.7449067   73.78652821]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  0.98763309 196.02708155  73.48011958]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  0.98713564 196.30758267  73.17552845]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  0.98663894 196.58641997  72.87274402]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  0.98614298 196.86360333  72.57175557]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  0.98564776 197.13914255  72.27255247]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  0.98515327 197.41304739  71.97512411]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  0.98465951 197.68532755  71.67945997]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  0.98416647 197.95599265  71.38554958]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  0.98367416 198.22505229  71.09338255]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  0.98318256 198.49251598  70.80294852]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  0.98269167 198.75839319  70.51423723]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  0.98220149 199.02269332  70.22723846]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  0.98171201 199.28542574  69.94194204]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  0.98122324 199.54659975  69.65833787]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  0.98073515 199.80622458  69.37641593]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  0.98024776 200.06430943  69.09616623]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  0.97976106 200.32086343  68.81757885]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  0.97927503 200.57589566  68.54064393]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  0.97878969 200.82941515  68.26535168]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  0.97830502 201.08143086  67.99169234]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  0.97782102 201.33195173  67.71965623]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  0.97733769 201.58098662  67.44923373]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  0.97685502 201.82854433  67.18041526]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  0.97637301 202.07463364  66.9131913 ]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  0.97589165 202.31926325  66.64755241]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  0.97541094 202.56244183  66.38348917]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  0.97493089 202.80417797  66.12099225]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  0.97445147 203.04448023  65.86005234]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  0.97397269 203.28335713  65.60066023]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  0.97349455 203.52081711  65.34280671]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  0.97301704 203.75686858  65.08648268]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  0.97254016 203.99151989  64.83167905]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  0.97206391 204.22477935  64.5783868 ]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  0.97158827 204.45665521  64.32659699]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  0.97111325 204.68715569  64.07630068]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  0.97063885 204.91628893  63.82748902]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  0.97016505 205.14406305  63.58015321]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  0.96969186 205.37048612  63.33428449]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  0.96921928 205.59556614  63.08987416]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  0.96874729 205.81931108  62.84691357]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  0.9682759  206.04172886  62.60539412]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  0.9678051  206.26282735  62.36530726]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  0.96733488 206.48261438  62.12664449]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  0.96686525 206.70109773  61.88939737]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  0.96639621 206.91828513  61.65355751]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  0.96592774 207.13418427  61.41911654]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  0.96545985 207.34880278  61.18606618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  0.96499252 207.56214827  60.95439817]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  0.96452577 207.77422828  60.72410432]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  0.96405958 207.98505033  60.49517648]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  0.96359394 208.19462186  60.26760654]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  0.96312887 208.40295031  60.04138645]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  0.96266435 208.61004305  59.81650821]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  0.96220039 208.8159074   59.59296384]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  0.96173697 209.02055064  59.37074545]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  0.96127409 209.22398004  59.14984516]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  0.96081176 209.42620277  58.93025516]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  0.96034996 209.62722601  58.71196767]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  0.9598887  209.82705686  58.49497498]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  0.95942797 210.0257024   58.27926939]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  0.95896777 210.22316966  58.06484328]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  0.9585081  210.41946563  57.85168905]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  0.95804894 210.61459725  57.63979916]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  0.95759031 210.80857144  57.42916612]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  0.95713219 211.00139506  57.21978246]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  0.95667459 211.19307492  57.01164078]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  0.95621749 211.38361783  56.80473371]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  0.95576091 211.57303051  56.59905392]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  0.95530482 211.76131968  56.39459414]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  0.95484924 211.948492    56.19134712]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  0.95439415 212.13455409  55.98930569]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  0.95393957 212.31951254  55.78846267]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  0.95348547 212.5033739   55.58881098]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  0.95303186 212.68614466  55.39034353]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  0.95257874 212.86783131  55.19305331]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  0.9521261  213.04844028  54.99693333]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  0.95167394 213.22797794  54.80197665]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  0.95122226 213.40645067  54.60817638]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  0.95077106 213.58386477  54.41552564]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  0.95032032 213.76022653  54.22401763]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  0.94987006 213.93554218  54.03364556]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  0.94942026 214.10981794  53.8444027 ]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  0.94897093 214.28305997  53.65628235]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  0.94852205 214.4552744   53.46927785]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  0.94807364 214.62646732  53.28338258]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  0.94762568 214.79664481  53.09858997]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  0.94717817 214.96581287  52.91489346]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  0.94673112 215.1339775   52.73228657]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  0.94628451 215.30114465  52.55076282]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  0.94583835 215.46732023  52.3703158 ]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  0.94539262 215.63251013  52.19093911]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  0.94494734 215.7967202   52.01262641]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  0.9445025  215.95995624  51.83537139]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  0.94405808 216.12222404  51.65916776]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  0.94361411 216.28352933  51.48400931]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  0.94317056 216.44387783  51.30988982]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  0.94272743 216.6032752   51.13680313]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  0.94228473 216.76172711  50.96474312]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  0.94184245 216.91923914  50.7937037 ]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  0.9414006  217.07581688  50.62367881]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  0.94095915 217.23146586  50.45466244]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  0.94051813 217.3861916   50.2866486 ]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  0.94007751 217.53999958  50.11963135]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  0.9396373  217.69289523  49.95360478]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  0.9391975  217.84488397  49.788563  ]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  0.93875811 217.99597118  49.62450019]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  0.93831911 218.1461622   49.46141052]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  0.93788052 218.29546235  49.29928824]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  0.93744232 218.44387692  49.1381276 ]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  0.93700452 218.59141116  48.97792289]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  0.93656711 218.73807028  48.81866845]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  0.93613009 218.88385949  48.66035864]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  0.93569346 219.02878394  48.50298786]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  0.93525721 219.17284875  48.34655054]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  0.93482135 219.31605904  48.19104113]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  0.93438587 219.45841986  48.03645415]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  0.93395077 219.59993626  47.88278411]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  0.93351604 219.74061325  47.73002557]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  0.93308169 219.8804558   47.57817313]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  0.93264771 220.01946886  47.42722142]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  0.9322141  220.15765735  47.27716509]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  0.93178086 220.29502618  47.12799884]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  0.93134798 220.43158018  46.97971737]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  0.93091547 220.56732421  46.83231545]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  0.93048332 220.70226306  46.68578785]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  0.93005152 220.83640152  46.5401294 ]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  0.92962009 220.96974432  46.39533492]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  0.92918901 221.10229618  46.25139931]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  0.92875828 221.23406181  46.10831747]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  0.9283279  221.36504586  45.96608432]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  0.92789787 221.49525296  45.82469485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  0.92746819 221.62468774  45.68414404]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  0.92703885 221.75335476  45.54442691]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  0.92660985 221.88125859  45.40553853]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  0.9261812  222.00840374  45.26747398]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  0.92575288 222.13479472  45.13022836]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  0.9253249  222.26043601  44.99379683]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  0.92489725 222.38533204  44.85817456]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  0.92446994 222.50948724  44.72335674]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  0.92404295 222.63290601  44.5893386 ]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  0.9236163  222.75559271  44.4561154 ]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  0.92318997 222.87755169  44.32368243]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  0.92276396 222.99878726  44.19203499]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  0.92233828 223.11930371  44.06116844]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  0.92191292 223.2391053   43.93107812]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  0.92148787 223.35819629  43.80175945]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  0.92106315 223.47658088  43.67320784]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  0.92063874 223.59426326  43.54541875]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  0.92021464 223.7112476   43.41838765]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  0.91979085 223.82753804  43.29211004]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  0.91936738 223.94313869  43.16658146]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  0.91894421 224.05805365  43.04179746]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  0.91852134 224.17228698  42.91775363]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  0.91809878 224.28584273  42.79444557]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  0.91767652 224.39872492  42.67186893]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  0.91725457 224.51093753  42.55001936]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  0.91683291 224.62248455  42.42889255]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  0.91641154 224.73336992  42.30848422]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  0.91599048 224.84359757  42.18879009]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  0.9155697  224.95317139  42.06980594]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  0.91514922 225.06209526  41.95152756]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  0.91472903 225.17037305  41.83395075]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  0.91430912 225.27800858  41.71707136]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  0.91388951 225.38500566  41.60088525]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  0.91347017 225.49136808  41.48538831]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  0.91305112 225.5970996   41.37057645]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  0.91263235 225.70220397  41.2564456 ]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  0.91221386 225.8066849   41.14299173]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  0.91179565 225.9105461   41.03021082]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  0.91137772 226.01379124  40.91809888]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  0.91096006 226.11642397  40.80665194]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  0.91054267 226.21844792  40.69586606]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  0.91012555 226.31986672  40.58573732]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  0.90970871 226.42068394  40.47626181]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  0.90929213 226.52090316  40.36743566]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  0.90887582 226.62052792  40.25925503]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  0.90845977 226.71956175  40.15171607]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  0.90804399 226.81800815  40.044815  ]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  0.90762847 226.91587061  39.93854802]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  0.90721321 227.01315259  39.83291136]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  0.90679821 227.10985753  39.7279013 ]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  0.90638347 227.20598887  39.62351412]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  0.90596898 227.30154999  39.51974612]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  0.90555474 227.39654428  39.41659363]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  0.90514076 227.49097511  39.31405299]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  0.90472703 227.58484582  39.21212058]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  0.90431355 227.67815972  39.1107928 ]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  0.90390032 227.77092013  39.01006604]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  0.90348734 227.86313032  38.90993676]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  0.9030746  227.95479356  38.8104014 ]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  0.9026621  228.0459131   38.71145644]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  0.90224985 228.13649215  38.61309839]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  0.90183784 228.22653392  38.51532375]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  0.90142606 228.31604161  38.41812907]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  0.90101453 228.40501837  38.32151091]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  0.90060323 228.49346736  38.22546585]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  0.90019217 228.58139171  38.12999049]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  0.89978134 228.66879453  38.03508145]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  0.89937074 228.75567891  37.94073537]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  0.89896038 228.84204794  37.84694891]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  0.89855024 228.92790465  37.75371875]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  0.89814034 229.01325211  37.6610416 ]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  0.89773066 229.09809331  37.56891417]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  0.8973212  229.18243128  37.4773332 ]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  0.89691197 229.26626899  37.38629545]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  0.89650297 229.34960941  37.29579769]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  0.89609418 229.43245549  37.20583674]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  0.89568562 229.51481016  37.11640939]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  0.89527727 229.59667634  37.02751249]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  0.89486915 229.67805693  36.93914289]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  0.89446124 229.7589548   36.85129746]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  0.89405354 229.83937281  36.7639731 ]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  0.89364606 229.91931383  36.6771667 ]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  0.89323879 229.99878066  36.59087521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  0.89283173 230.07777613  36.50509556]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  0.89242489 230.15630304  36.41982472]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  0.89201825 230.23436416  36.33505967]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  0.89161182 230.31196225  36.25079741]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  0.8912056  230.38910006  36.16703496]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  0.89079958 230.46578033  36.08376936]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  0.89039376 230.54200576  36.00099765]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  0.88998815 230.61777905  35.91871691]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  0.88958274 230.69310289  35.83692423]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  0.88917753 230.76797994  35.7556167 ]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  0.88877252 230.84241285  35.67479146]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  0.88836771 230.91640426  35.59444564]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  0.88796309 230.98995678  35.5145764 ]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  0.88755867 231.06307302  35.43518091]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  0.88715445 231.13575556  35.35625636]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  0.88675041 231.20800698  35.27779996]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  0.88634658 231.27982984  35.19980892]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  0.88594293 231.35122668  35.1222805 ]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  0.88553947 231.42220002  35.04521195]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  0.8851362  231.49275237  34.96860054]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  0.88473312 231.56288624  34.89244355]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  0.88433022 231.63260411  34.81673829]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  0.88392751 231.70190844  34.74148209]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  0.88352498 231.77080168  34.66667227]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  0.88312264 231.83928628  34.5923062 ]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  0.88272048 231.90736466  34.51838123]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  0.8823185  231.97503923  34.44489476]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  0.8819167  232.04231238  34.37184418]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  0.88151509 232.10918649  34.2992269 ]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  0.88111364 232.17566394  34.22704036]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  0.88071238 232.24174707  34.155282  ]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  0.88031129 232.30743822  34.08394928]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  0.87991037 232.37273972  34.01303968]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  0.87950963 232.43765388  33.94255069]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  0.87910907 232.502183    33.8724798 ]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  0.87870867 232.56632936  33.80282455]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  0.87830844 232.63009523  33.73358246]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  0.87790839 232.69348286  33.66475109]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  0.8775085  232.75649451  33.596328  ]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  0.87710878 232.8191324   33.52831077]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  0.87670923 232.88139875  33.46069699]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  0.87630984 232.94329576  33.39348426]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  0.87591062 233.00482563  33.32667021]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  0.87551156 233.06599052  33.26025248]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  0.87511266 233.12679262  33.1942287 ]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  0.87471393 233.18723406  33.12859656]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  0.87431536 233.24731698  33.06335372]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  0.87391694 233.30704353  32.99849787]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  0.87351869 233.36641579  32.93402671]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  0.87312059 233.42543589  32.86993798]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  0.87272265 233.48410591  32.80622939]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  0.87232487 233.54242792  32.7428987 ]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  0.87192724 233.60040399  32.67994366]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  0.87152977 233.65803617  32.61736204]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  0.87113244 233.7153265   32.55515163]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  0.87073528 233.772277    32.49331022]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  0.87033826 233.82888971  32.43183564]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  0.86994139 233.88516661  32.37072569]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  0.86954468 233.94110969  32.30997823]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  0.86914811 233.99672095  32.24959109]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  0.86875169 234.05200235  32.18956215]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  0.86835542 234.10695584  32.12988927]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  0.86795929 234.16158337  32.07057035]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  0.86756331 234.21588687  32.01160328]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  0.86716747 234.26986827  31.95298599]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  0.86677178 234.32352947  31.89471638]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  0.86637623 234.37687238  31.83679241]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  0.86598082 234.42989888  31.77921202]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  0.86558556 234.48261085  31.72197317]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  0.86519043 234.53501016  31.66507384]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  0.86479544 234.58709865  31.60851201]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  0.86440059 234.63887818  31.55228569]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  0.86400588 234.69035057  31.49639287]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  0.86361131 234.74151765  31.44083159]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  0.86321687 234.79238123  31.38559988]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  0.86282257 234.84294311  31.33069578]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  0.8624284  234.89320508  31.27611734]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  0.86203437 234.94316891  31.22186265]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  0.86164046 234.99283639  31.16792977]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  0.86124669 235.04220925  31.1143168 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  0.86085306 235.09128926  31.06102184]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  0.86045955 235.14007814  31.008043  ]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  0.86006617 235.18857763  30.95537841]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  0.85967292 235.23678944  30.90302621]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  0.8592798  235.28471528  30.85098453]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  0.85888681 235.33235684  30.79925155]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  0.85849395 235.37971582  30.74782543]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  0.85810121 235.42679388  30.69670435]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  0.85770859 235.47359269  30.64588649]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  0.8573161  235.52011392  30.59537007]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  0.85692373 235.5663592   30.54515329]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  0.85653149 235.61233017  30.49523437]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  0.85613937 235.65802846  30.44561155]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  0.85574737 235.70345569  30.39628308]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  0.85535549 235.74861346  30.3472472 ]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  0.85496374 235.79350338  30.29850218]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  0.8545721  235.83812703  30.2500463 ]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  0.85418058 235.88248599  30.20187784]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  0.85378917 235.92658183  30.1539951 ]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  0.85339789 235.97041612  30.10639637]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  0.85300672 236.0139904   30.05907999]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  0.85261567 236.05730621  30.01204426]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  0.85222473 236.1003651   29.96528753]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  0.85183391 236.14316857  29.91880814]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  0.8514432  236.18571816  29.87260445]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  0.85105261 236.22801536  29.82667482]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  0.85066212 236.27006167  29.78101763]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  0.85027175 236.31185858  29.73563126]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  0.84988149 236.35340757  29.6905141 ]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  0.84949134 236.3947101   29.64566455]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  0.84910131 236.43576765  29.60108104]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  0.84871138 236.47658166  29.55676198]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  0.84832155 236.51715358  29.5127058 ]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  0.84793184 236.55748485  29.46891094]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  0.84754224 236.59757689  29.42537586]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  0.84715274 236.63743112  29.38209901]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  0.84676334 236.67704895  29.33907886]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  0.84637405 236.71643178  29.29631389]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  0.84598487 236.75558101  29.25380259]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  0.84559579 236.79449803  29.21154344]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  0.84520682 236.83318421  29.16953496]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  0.84481794 236.87164091  29.12777565]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  0.84442917 236.90986951  29.08626405]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  0.84404051 236.94787135  29.04499867]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  0.84365194 236.98564779  29.00397806]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  0.84326347 237.02320014  28.96320078]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  0.84287511 237.06052976  28.92266536]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  0.84248684 237.09763795  28.88237039]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  0.84209867 237.13452603  28.84231443]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  0.8417106  237.1711953   28.80249606]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  0.84132263 237.20764707  28.76291389]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  0.84093475 237.24388263  28.72356649]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  0.84054697 237.27990325  28.68445249]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  0.84015929 237.31571021  28.6455705 ]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  0.8397717  237.35130478  28.60691914]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  0.8393842  237.38668821  28.56849705]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  0.8389968  237.42186177  28.53030286]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  0.8386095  237.45682669  28.49233522]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  0.83822228 237.49158422  28.45459279]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  0.83783516 237.52613558  28.41707424]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  0.83744814 237.56048199  28.37977823]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  0.8370612  237.59462468  28.34270344]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  0.83667435 237.62856485  28.30584857]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  0.8362876  237.6623037   28.26921231]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  0.83590093 237.69584242  28.23279335]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  0.83551435 237.72918221  28.19659042]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  0.83512786 237.76232424  28.16060224]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  0.83474146 237.79526968  28.12482751]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  0.83435515 237.82801971  28.08926499]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  0.83396893 237.86057548  28.05391342]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  0.83358279 237.89293814  28.01877153]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  0.83319673 237.92510884  27.98383809]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  0.83281077 237.95708871  27.94911187]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  0.83242489 237.9888789   27.91459162]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  0.83203909 238.02048052  27.88027614]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  0.83165338 238.05189469  27.8461642 ]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  0.83126775 238.08312253  27.8122546 ]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  0.8308822  238.11416513  27.77854614]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  0.83049673 238.14502361  27.74503762]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  0.83011135 238.17569904  27.71172786]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  0.82972605 238.20619253  27.67861569]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  0.82934083 238.23650513  27.64569992]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  0.82895569 238.26663794  27.61297939]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  0.82857064 238.29659201  27.58045295]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  0.82818566 238.32636841  27.54811945]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  0.82780076 238.35596818  27.51597773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  0.82741594 238.38539239  27.48402666]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  0.8270312  238.41464206  27.45226512]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  0.82664653 238.44371823  27.42069197]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  0.82626194 238.47262194  27.3893061 ]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  0.82587743 238.5013542   27.35810641]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  0.825493   238.52991604  27.32709177]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  0.82510864 238.55830846  27.2962611 ]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  0.82472436 238.58653246  27.26561331]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  0.82434015 238.61458906  27.23514731]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  0.82395602 238.64247923  27.20486202]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  0.82357196 238.67020397  27.17475637]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  0.82318798 238.69776426  27.14482929]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  0.82280407 238.72516108  27.11507973]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  0.82242023 238.75239539  27.08550664]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  0.82203646 238.77946815  27.05610896]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  0.82165277 238.80638033  27.02688565]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  0.82126915 238.83313288  26.99783569]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  0.8208856  238.85972675  26.96895804]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  0.82050212 238.88616287  26.94025168]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  0.82011871 238.91244219  26.91171559]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  0.81973537 238.93856562  26.88334877]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  0.81935209 238.96453411  26.85515021]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  0.81896889 238.99034856  26.82711891]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  0.81858576 239.01600989  26.79925389]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  0.8182027  239.041519    26.77155415]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  0.8178197  239.06687681  26.74401871]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  0.81743677 239.09208421  26.7166466 ]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  0.81705391 239.11714209  26.68943685]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  0.81667111 239.14205133  26.6623885 ]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  0.81628838 239.16681282  26.63550059]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  0.81590572 239.19142744  26.60877217]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  0.81552312 239.21589606  26.58220229]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  0.81514059 239.24021954  26.55579002]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  0.81475812 239.26439874  26.52953441]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  0.81437572 239.28843452  26.50343454]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  0.81399338 239.31232773  26.47748948]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  0.8136111  239.33607922  26.45169832]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  0.81322889 239.35968982  26.42606014]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  0.81284674 239.38316038  26.40057404]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  0.81246465 239.40649171  26.37523911]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  0.81208263 239.42968466  26.35005446]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  0.81170067 239.45274004  26.32501919]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  0.81131876 239.47565866  26.30013243]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  0.81093692 239.49844133  26.27539328]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  0.81055514 239.52108887  26.25080087]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  0.81017342 239.54360208  26.22635434]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  0.80979176 239.56598175  26.20205282]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  0.80941016 239.58822867  26.17789544]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  0.80902862 239.61034363  26.15388135]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  0.80864713 239.63232741  26.13000971]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  0.80826571 239.6541808   26.10627966]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  0.80788434 239.67590456  26.08269038]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  0.80750303 239.69749946  26.05924101]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  0.80712178 239.71896627  26.03593074]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  0.80674059 239.74030575  26.01275873]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  0.80635945 239.76151866  25.98972417]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  0.80597837 239.78260574  25.96682625]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  0.80559735 239.80356774  25.94406414]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  0.80521638 239.8244054   25.92143706]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  0.80483546 239.84511946  25.89894418]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  0.80445461 239.86571065  25.87658473]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  0.8040738  239.88617971  25.8543579 ]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  0.80369305 239.90652735  25.83226292]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  0.80331236 239.9267543   25.81029899]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  0.80293172 239.94686127  25.78846535]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  0.80255113 239.96684898  25.76676121]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  0.8021706  239.98671812  25.74518582]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  0.80179011 240.00646941  25.7237384 ]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  0.80140969 240.02610355  25.7024182 ]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  0.80102931 240.04562122  25.68122446]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  0.80064898 240.06502313  25.66015644]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  0.80026871 240.08430995  25.63921338]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  0.79988849 240.10348236  25.61839455]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  0.79950832 240.12254106  25.59769921]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  0.7991282  240.14148671  25.57712663]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  0.79874813 240.16031998  25.55667607]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  0.79836811 240.17904154  25.53634682]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  0.79798814 240.19765205  25.51613816]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  0.79760822 240.21615217  25.49604936]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  0.79722835 240.23454256  25.47607973]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  0.79684853 240.25282386  25.45622854]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  0.79646875 240.27099673  25.43649511]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  0.79608903 240.2890618   25.41687873]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  0.79570935 240.30701972  25.3973787 ]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  0.79532972 240.32487112  25.37799435]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  0.79495014 240.34261663  25.35872497]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  0.7945706  240.36025688  25.33956989]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  0.79419112 240.3777925   25.32052843]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  0.79381168 240.39522411  25.30159992]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  0.79343228 240.41255232  25.28278369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  0.79305293 240.42977774  25.26407906]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  0.79267363 240.44690099  25.24548539]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  0.79229437 240.46392268  25.227002  ]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  0.79191516 240.4808434   25.20862825]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  0.79153599 240.49766375  25.19036349]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  0.79115687 240.51438433  25.17220707]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  0.79077779 240.53100573  25.15415835]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  0.79039876 240.54752854  25.13621668]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  0.79001977 240.56395335  25.11838144]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  0.78964083 240.58028072  25.10065199]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  0.78926192 240.59651126  25.0830277 ]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  0.78888307 240.61264552  25.06550795]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  0.78850425 240.62868408  25.04809213]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  0.78812548 240.6446275   25.03077961]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  0.78774675 240.66047635  25.01356978]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  0.78736806 240.6762312   24.99646203]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  0.78698941 240.6918926   24.97945576]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  0.7866108  240.70746109  24.96255036]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  0.78623224 240.72293725  24.94574524]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  0.78585372 240.7383216   24.92903981]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  0.78547524 240.7536147   24.91243346]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  0.7850968  240.76881709  24.89592562]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  0.78471839 240.78392931  24.87951569]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  0.78434003 240.79895188  24.8632031 ]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  0.78396171 240.81388535  24.84698728]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  0.78358343 240.82873024  24.83086763]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  0.78320519 240.84348708  24.81484361]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  0.78282699 240.85815638  24.79891463]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  0.78244883 240.87273868  24.78308014]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  0.7820707  240.88723447  24.76733957]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  0.78169262 240.90164429  24.75169237]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  0.78131457 240.91596863  24.73613798]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  0.78093656 240.930208    24.72067586]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  0.78055859 240.94436292  24.70530545]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  0.78018066 240.95843387  24.69002622]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  0.77980276 240.97242136  24.67483761]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  0.7794249  240.98632588  24.6597391 ]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  0.77904708 241.00014793  24.64473014]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  0.7786693  241.01388799  24.62981022]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  0.77829155 241.02754655  24.61497879]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  0.77791384 241.04112409  24.60023534]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  0.77753616 241.0546211   24.58557934]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  0.77715852 241.06803805  24.57101028]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  0.77678092 241.08137541  24.55652763]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  0.77640335 241.09463367  24.54213089]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  0.77602581 241.10781328  24.52781955]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  0.77564832 241.12091471  24.51359309]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  0.77527085 241.13393844  24.49945102]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  0.77489342 241.14688491  24.48539284]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  0.77451603 241.15975459  24.47141805]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  0.77413867 241.17254793  24.45752615]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  0.77376134 241.18526539  24.44371665]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  0.77338405 241.19790741  24.42998906]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  0.77300679 241.21047444  24.4163429 ]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  0.77262957 241.22296694  24.40277769]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  0.77225237 241.23538533  24.38929294]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  0.77187521 241.24773006  24.37588817]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  0.77149809 241.26000157  24.36256292]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  0.77112099 241.27220029  24.34931671]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  0.77074393 241.28432665  24.33614907]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  0.7703669  241.29638109  24.32305954]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  0.76998991 241.30836402  24.31004765]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  0.76961294 241.32027587  24.29711294]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  0.76923601 241.33211707  24.28425495]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  0.76885911 241.34388803  24.27147324]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  0.76848224 241.35558918  24.25876734]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  0.7681054  241.36722091  24.2461368 ]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  0.76772859 241.37878365  24.23358119]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  0.76735181 241.39027781  24.22110005]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  0.76697507 241.40170378  24.20869295]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  0.76659835 241.41306199  24.19635944]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  0.76622166 241.42435282  24.18409909]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  0.76584501 241.43557668  24.17191147]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  0.76546838 241.44673396  24.15979613]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  0.76509178 241.45782506  24.14775266]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  0.76471522 241.46885038  24.13578063]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  0.76433868 241.4798103   24.12387961]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  0.76396217 241.49070521  24.11204919]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  0.76358569 241.50153549  24.10028894]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  0.76320924 241.51230153  24.08859844]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  0.76283282 241.52300372  24.07697729]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  0.76245642 241.53364242  24.06542508]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  0.76208006 241.54421802  24.05394138]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  0.76170372 241.55473089  24.04252581]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  0.76132741 241.5651814   24.03117794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  0.76095113 241.57556993  24.01989739]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  0.76057488 241.58589683  24.00868375]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  0.76019865 241.59616248  23.99753663]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  0.75982245 241.60636724  23.98645563]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  0.75944628 241.61651147  23.97544036]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  0.75907013 241.62659552  23.96449042]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  0.75869402 241.63661976  23.95360544]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  0.75831792 241.64658455  23.94278502]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  0.75794186 241.65649022  23.93202879]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  0.75756582 241.66633714  23.92133635]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  0.75718981 241.67612565  23.91070735]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  0.75681382 241.6858561   23.90014139]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  0.75643786 241.69552884  23.8896381 ]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  0.75606193 241.7051442   23.87919711]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  0.75568602 241.71470252  23.86881806]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  0.75531014 241.72420415  23.85850058]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  0.75493428 241.73364942  23.84824429]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  0.75455844 241.74303866  23.83804884]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  0.75418264 241.75237222  23.82791386]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  0.75380685 241.7616504   23.81783901]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  0.75343109 241.77087356  23.80782391]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  0.75305536 241.780042    23.79786822]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  0.75267965 241.78915607  23.78797158]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  0.75230396 241.79821607  23.77813365]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  0.7519283  241.80722233  23.76835407]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  0.75155266 241.81617517  23.75863251]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  0.75117705 241.82507491  23.7489686 ]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  0.75080146 241.83392186  23.73936202]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  0.75042589 241.84271633  23.72981243]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  0.75005035 241.85145864  23.72031948]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  0.74967483 241.86014909  23.71088283]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  0.74929934 241.86878799  23.70150217]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  0.74892386 241.87737565  23.69217714]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  0.74854841 241.88591237  23.68290743]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  0.74817298 241.89439846  23.6736927 ]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  0.74779758 241.90283421  23.66453264]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  0.74742219 241.91121992  23.6554269 ]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  0.74704683 241.91955589  23.64637518]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  0.7466715  241.92784242  23.63737715]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  0.74629618 241.9360798   23.6284325 ]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  0.74592088 241.94426831  23.6195409 ]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  0.74554561 241.95240825  23.61070204]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  0.74517036 241.96049991  23.60191562]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  0.74479513 241.96854358  23.59318131]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  0.74441992 241.97653953  23.58449881]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  0.74404474 241.98448806  23.57586781]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  0.74366957 241.99238943  23.56728801]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  0.74329443 242.00024394  23.5587591 ]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  0.7429193  242.00805186  23.55028078]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  0.7425442  242.01581347  23.54185275]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  0.74216912 242.02352904  23.53347472]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  0.74179405 242.03119884  23.52514638]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  0.74141901 242.03882315  23.51686744]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  0.74104399 242.04640224  23.50863761]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  0.74066899 242.05393637  23.5004566 ]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  0.74029401 242.06142581  23.49232411]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  0.73991905 242.06887082  23.48423987]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  0.73954411 242.07627168  23.47620357]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  0.73916919 242.08362863  23.46821495]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  0.73879428 242.09094195  23.46027371]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  0.7384194  242.09821189  23.45237958]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  0.73804454 242.10543871  23.44453227]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  0.73766969 242.11262266  23.43673151]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  0.73729487 242.119764    23.42897702]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  0.73692006 242.12686298  23.42126853]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  0.73654528 242.13391985  23.41360576]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  0.73617051 242.14093486  23.40598845]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  0.73579576 242.14790826  23.39841632]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  0.73542103 242.1548403   23.39088911]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  0.73504632 242.16173122  23.38340654]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  0.73467162 242.16858127  23.37596836]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  0.73429695 242.17539069  23.3685743 ]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  0.73392229 242.18215971  23.3612241 ]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  0.73354765 242.18888859  23.3539175 ]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  0.73317303 242.19557755  23.34665424]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  0.73279842 242.20222683  23.33943407]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  0.73242384 242.20883668  23.33225672]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  0.73204927 242.21540732  23.32512194]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  0.73167472 242.22193898  23.31802949]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  0.73130018 242.2284319   23.31097911]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  0.73092566 242.23488631  23.30397054]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  0.73055117 242.24130243  23.29700355]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  0.73017668 242.2476805   23.29007789]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  0.72980222 242.25402073  23.28319331]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  0.72942777 242.26032336  23.27634956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  0.72905334 242.2665886   23.26954641]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  0.72867892 242.27281668  23.26278362]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  0.72830452 242.27900781  23.25606094]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  0.72793014 242.28516223  23.24937813]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  0.72755578 242.29128014  23.24273497]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  0.72718143 242.29736175  23.23613121]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  0.72680709 242.3034073   23.22956663]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  0.72643278 242.30941699  23.22304098]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  0.72605847 242.31539103  23.21655404]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  0.72568419 242.32132963  23.21010558]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  0.72530992 242.32723301  23.20369537]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  0.72493567 242.33310137  23.19732319]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  0.72456143 242.33893492  23.1909888 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  0.72418721 242.34473388  23.18469198]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  0.723813   242.35049843  23.17843252]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  0.72343881 242.35622879  23.17221019]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  0.72306463 242.36192516  23.16602476]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  0.72269047 242.36758775  23.15987603]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  0.72231632 242.37321674  23.15376377]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  0.72194219 242.37881235  23.14768776]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  0.72156807 242.38437477  23.1416478 ]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  0.72119397 242.38990419  23.13564366]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  0.72081989 242.39540081  23.12967514]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  0.72044581 242.40086483  23.12374202]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  0.72007176 242.40629644  23.1178441 ]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  0.71969771 242.41169584  23.11198116]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  0.71932368 242.4170632   23.106153  ]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  0.71894967 242.42239873  23.10035941]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  0.71857567 242.42770261  23.09460018]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  0.71820168 242.43297503  23.08887512]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  0.71782771 242.43821618  23.08318402]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  0.71745375 242.44342624  23.07752667]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  0.71707981 242.44860539  23.07190289]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  0.71670588 242.45375382  23.06631246]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  0.71633196 242.45887172  23.06075519]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  0.71595806 242.46395926  23.05523089]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  0.71558417 242.46901662  23.04973935]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  0.71521029 242.47404398  23.0442804 ]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  0.71483643 242.47904153  23.03885382]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  0.71446258 242.48400943  23.03345943]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  0.71408874 242.48894786  23.02809704]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  0.71371492 242.493857    23.02276645]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  0.71334111 242.49873702  23.01746749]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  0.71296731 242.5035881   23.01219996]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  0.71259352 242.5084104   23.00696367]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  0.71221975 242.51320409  23.00175845]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  0.71184599 242.51796936  22.9965841 ]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  0.71147225 242.52270635  22.99144045]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  0.71109851 242.52741525  22.9863273 ]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  0.71072479 242.53209622  22.98124449]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  0.71035108 242.53674942  22.97619182]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  0.70997739 242.54137503  22.97116913]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  0.7096037  242.54597319  22.96617623]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  0.70923003 242.55054408  22.96121294]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  0.70885637 242.55508786  22.9562791 ]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  0.70848273 242.55960468  22.95137453]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  0.70810909 242.56409472  22.94649904]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  0.70773547 242.56855812  22.94165248]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  0.70736185 242.57299505  22.93683466]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  0.70698826 242.57740566  22.93204543]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  0.70661467 242.5817901   22.9272846 ]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  0.70624109 242.58614854  22.92255201]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  0.70586753 242.59048113  22.9178475 ]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  0.70549397 242.59478802  22.91317089]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  0.70512043 242.59906936  22.90852202]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  0.7047469  242.60332531  22.90390073]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  0.70437338 242.60755601  22.89930685]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  0.70399987 242.61176162  22.89474022]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  0.70362638 242.61594228  22.89020068]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  0.70325289 242.62009815  22.88568807]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  0.70287942 242.62422936  22.88120222]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  0.70250595 242.62833607  22.87674298]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  0.7021325  242.63241842  22.8723102 ]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  0.70175906 242.63647655  22.86790371]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  0.70138563 242.64051062  22.86352335]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  0.70101221 242.64452075  22.85916898]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  0.7006388  242.6485071   22.85484044]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  0.7002654  242.65246981  22.85053758]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  0.69989201 242.65640901  22.84626024]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  0.69951863 242.66032484  22.84200827]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  0.69914527 242.66421745  22.83778153]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  0.69877191 242.66808696  22.83357985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  0.69839856 242.67193353  22.8294031 ]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  0.69802523 242.67575728  22.82525112]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  0.6976519  242.67955834  22.82112378]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  0.69727858 242.68333687  22.81702091]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  0.69690528 242.68709297  22.81294239]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  0.69653198 242.6908268   22.80888806]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  0.6961587  242.69453849  22.80485777]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  0.69578542 242.69822815  22.80085139]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  0.69541215 242.70189593  22.79686878]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  0.6950389  242.70554196  22.79290979]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  0.69466565 242.70916636  22.78897429]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  0.69429241 242.71276926  22.78506213]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  0.69391918 242.71635078  22.78117318]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  0.69354597 242.71991107  22.77730729]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  0.69317276 242.72345024  22.77346434]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  0.69279956 242.72696841  22.76964418]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  0.69242637 242.73046572  22.76584669]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  0.69205319 242.73394228  22.76207172]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  0.69168002 242.73739822  22.75831914]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  0.69130685 242.74083366  22.75458882]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  0.6909337  242.74424872  22.75088063]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  0.69056056 242.74764353  22.74719444]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  0.69018742 242.7510182   22.74353011]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  0.68981429 242.75437285  22.73988752]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  0.68944118 242.75770761  22.73626653]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  0.68906807 242.76102258  22.73266703]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  0.68869497 242.76431789  22.72908887]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  0.68832188 242.76759366  22.72553194]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  0.6879488  242.77084999  22.72199611]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  0.68757572 242.77408701  22.71848126]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  0.68720266 242.77730483  22.71498725]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  0.6868296  242.78050356  22.71151397]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  0.68645656 242.78368332  22.70806129]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  0.68608352 242.78684422  22.70462909]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  0.68571049 242.78998637  22.70121726]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  0.68533746 242.79310988  22.69782566]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  0.68496445 242.79621486  22.69445418]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  0.68459144 242.79930143  22.6911027 ]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  0.68421845 242.80236969  22.6877711 ]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  0.68384546 242.80541975  22.68445926]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  0.68347248 242.80845171  22.68116706]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  0.6830995  242.81146569  22.6778944 ]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  0.68272654 242.8144618   22.67464115]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  0.68235358 242.81744013  22.6714072 ]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  0.68198063 242.8204008   22.66819243]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  0.68160769 242.82334391  22.66499673]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  0.68123476 242.82626955  22.66181998]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  0.68086183 242.82917785  22.65866208]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  0.68048891 242.83206889  22.65552291]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  0.680116   242.83494279  22.65240237]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  0.6797431  242.83779964  22.64930033]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  0.6793702  242.84063954  22.6462167 ]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  0.67899732 242.8434626   22.64315135]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  0.67862444 242.84626891  22.6401042 ]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  0.67825156 242.84905858  22.63707511]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  0.6778787  242.8518317   22.63406399]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  0.67750584 242.85458837  22.63107074]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  0.67713299 242.85732869  22.62809524]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  0.67676015 242.86005276  22.62513739]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  0.67638731 242.86276067  22.62219709]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  0.67601448 242.86545251  22.61927423]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  0.67564166 242.86812839  22.61636871]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  0.67526885 242.8707884   22.61348042]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  0.67489604 242.87343263  22.61060926]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  0.67452324 242.87606117  22.60775514]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  0.67415045 242.87867413  22.60491795]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  0.67377766 242.88127158  22.60209758]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  0.67340488 242.88385363  22.59929395]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  0.67303211 242.88642036  22.59650695]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  0.67265934 242.88897187  22.59373648]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  0.67228658 242.89150824  22.59098244]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  0.67191383 242.89402957  22.58824474]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  0.67154109 242.89653594  22.58552328]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [  0.67116835 242.89902745  22.58281797]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [  0.67079562 242.90150417  22.5801287 ]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [  0.67042289 242.90396621  22.57745539]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [  0.67005017 242.90641364  22.57479793]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [  0.66967746 242.90884656  22.57215623]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [  0.66930475 242.91126504  22.56953021]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [  0.66893205 242.91366918  22.56691977]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [  0.66855936 242.91605906  22.5643248 ]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [  0.66818667 242.91843476  22.56174524]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [  0.66781399 242.92079638  22.55918097]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [  0.66744132 242.92314398  22.55663192]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [  0.66706865 242.92547766  22.55409798]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [  0.66669599 242.92779749  22.55157908]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [  0.66632333 242.93010357  22.54907512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [  0.66595068 242.93239597  22.54658601]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [  0.66557804 242.93467477  22.54411167]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [  0.6652054  242.93694005  22.54165201]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [  0.66483277 242.9391919   22.53920693]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [  0.66446015 242.94143039  22.53677636]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [  0.66408753 242.9436556   22.53436021]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [  0.66371491 242.94586761  22.53195839]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [  0.66334231 242.9480665   22.52957082]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [  0.6629697  242.95025235  22.52719741]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [  0.66259711 242.95242524  22.52483808]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [  0.66222452 242.95458523  22.52249275]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [  0.66185193 242.95673242  22.52016133]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [  0.66147936 242.95886686  22.51784373]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [  0.66110678 242.96098865  22.51553989]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [  0.66073422 242.96309785  22.51324971]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [  0.66036165 242.96519454  22.51097312]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [  0.6599891  242.96727879  22.50871003]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [  0.65961655 242.96935068  22.50646037]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [  0.659244   242.97141029  22.50422405]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [  0.65887146 242.97345767  22.502001  ]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [  0.65849893 242.97549291  22.49979114]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [  0.6581264  242.97751608  22.49759438]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [  0.65775388 242.97952725  22.49541066]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [  0.65738136 242.98152648  22.49323989]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [  0.65700885 242.98351386  22.49108199]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [  0.65663634 242.98548946  22.4889369 ]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [  0.65626384 242.98745333  22.48680453]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [  0.65589134 242.98940555  22.48468481]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [  0.65551885 242.9913462   22.48257767]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [  0.65514636 242.99327533  22.48048302]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [  0.65477388 242.99519302  22.4784008 ]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [  0.6544014  242.99709934  22.47633093]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [  0.65402893 242.99899435  22.47427334]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [  0.65365647 243.00087811  22.47222796]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [  0.65328401 243.00275071  22.47019471]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [  0.65291155 243.0046122   22.46817352]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [  0.6525391  243.00646264  22.46616432]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [  0.65216665 243.00830211  22.46416704]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [  0.65179421 243.01013067  22.46218161]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [  0.65142178 243.01194838  22.46020795]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [  0.65104934 243.01375531  22.45824601]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [  0.65067692 243.01555152  22.4562957 ]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [  0.65030449 243.01733708  22.45435696]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [  0.64993208 243.01911205  22.45242972]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [  0.64955966 243.02087649  22.45051391]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [  0.64918726 243.02263046  22.44860947]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [  0.64881485 243.02437403  22.44671633]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [  0.64844246 243.02610726  22.44483441]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [  0.64807006 243.02783021  22.44296366]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [  0.64769767 243.02954293  22.44110401]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [  0.64732529 243.0312455   22.43925539]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [  0.64695291 243.03293797  22.43741773]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [  0.64658053 243.0346204   22.43559098]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [  0.64620816 243.03629285  22.43377506]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [  0.64583579 243.03795538  22.43196992]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [  0.64546343 243.03960805  22.43017548]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [  0.64509107 243.04125091  22.42839169]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [  0.64471872 243.04288403  22.42661848]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [  0.64434637 243.04450747  22.42485579]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [  0.64397402 243.04612127  22.42310355]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [  0.64360168 243.04772551  22.42136171]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [  0.64322935 243.04932022  22.4196302 ]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [  0.64285702 243.05090548  22.41790896]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [  0.64248469 243.05248133  22.41619794]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  0.64211236 243.05404784  22.41449706]\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix, \n",
    "                                                             output, inital_weights=initial_weights,\n",
    "                                                             setp_size=step_size, l2_penalty=0, \n",
    "                                                             max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [ 1.0186113  47.79222672 42.87728289]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [ 1.03094706 70.09427275 62.45135027]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [ 1.04031841 80.78561077 71.53154388]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [ 1.04828959 85.95953215 75.68834888]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [ 1.05559958 88.50142758 77.54648351]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [ 1.06259744 89.77973263 78.34037315]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [ 1.06944804 90.44511034 78.6487485 ]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [ 1.07622923 90.80825387 78.74147439]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [ 1.08297775 91.01855851 78.7432224 ]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [ 1.08971094 91.14867625 78.71064862]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [ 1.09643698 91.2345749  78.6687371 ]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [ 1.10315969 91.29455218 78.62787542]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [ 1.10988089 91.33828879 78.59184661]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [ 1.11660141 91.37118094 78.56154489]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [ 1.12332164 91.39643122 78.53668357]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [ 1.13004177 91.41607132 78.51656406]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [ 1.13676186 91.43147293 78.50040915]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [ 1.14348195 91.44361111 78.48749654]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [ 1.15020206 91.45320622 78.47720302]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [ 1.15692219 91.46080477 78.46901025]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [ 1.16364233 91.4668287  78.46249556]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [ 1.17036248 91.47160737 78.45731806]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [ 1.17708265 91.47539961 78.4532046 ]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [ 1.18380283 91.47840971 78.4499371 ]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [ 1.19052301 91.48079927 78.44734186]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [ 1.1972432  91.48269634 78.44528066]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [ 1.2039634  91.48420244 78.44364364]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [ 1.2106836  91.48539816 78.44234351]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [ 1.2174038  91.48634743 78.4413109 ]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [ 1.224124   91.48710102 78.44049074]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [ 1.23084421 91.48769923 78.43983929]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [ 1.23756442 91.48817406 78.4393218 ]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [ 1.24428462 91.48855093 78.4389107 ]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [ 1.25100483 91.48885    78.43858406]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [ 1.25772505 91.4890873  78.43832451]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [ 1.26444526 91.48927554 78.43811821]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [ 1.27116547 91.48942484 78.43795422]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [ 1.27788568 91.4895432  78.4378238 ]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [ 1.28460589 91.48963701 78.43772006]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [ 1.2913261  91.48971131 78.4376375 ]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [ 1.29804632 91.48977013 78.43757175]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [ 1.30476653 91.48981665 78.43751935]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [ 1.31148674 91.48985341 78.43747756]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [ 1.31820695 91.48988241 78.43744419]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [ 1.32492717 91.48990525 78.4374175 ]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [ 1.33164738 91.48992321 78.43739612]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [ 1.33836759 91.48993728 78.43737896]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [ 1.3450878  91.48994827 78.43736515]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [ 1.35180801 91.48995682 78.43735399]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [ 1.35852823 91.48996341 78.43734495]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [ 1.36524844 91.48996847 78.43733758]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [ 1.37196865 91.4899723  78.43733154]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [ 1.37868886 91.48997515 78.43732656]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [ 1.38540907 91.48997724 78.43732241]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [ 1.39212928 91.48997871 78.43731894]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [ 1.3988495  91.48997969 78.43731599]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [ 1.40556971 91.48998028 78.43731347]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [ 1.41228992 91.48998057 78.43731127]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [ 1.41901013 91.48998062 78.43730935]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [ 1.42573034 91.48998047 78.43730763]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [ 1.43245055 91.48998016 78.43730608]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [ 1.43917076 91.48997974 78.43730466]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [ 1.44589097 91.48997921 78.43730335]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [ 1.45261118 91.48997861 78.43730212]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [ 1.45933139 91.48997795 78.43730096]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [ 1.4660516  91.48997724 78.43729985]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [ 1.47277181 91.4899765  78.43729878]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [ 1.47949202 91.48997572 78.43729775]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [ 1.48621223 91.48997491 78.43729674]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [ 1.49293244 91.48997409 78.43729575]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [ 1.49965265 91.48997325 78.43729478]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [ 1.50637286 91.4899724  78.43729383]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [ 1.51309307 91.48997154 78.43729288]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [ 1.51981328 91.48997068 78.43729194]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [ 1.52653349 91.4899698  78.43729101]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [ 1.5332537  91.48996892 78.43729008]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [ 1.53997391 91.48996804 78.43728916]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [ 1.54669412 91.48996716 78.43728824]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [ 1.55341433 91.48996627 78.43728733]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [ 1.56013454 91.48996538 78.43728641]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [ 1.56685475 91.48996449 78.4372855 ]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [ 1.57357495 91.4899636  78.43728459]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [ 1.58029516 91.4899627  78.43728368]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [ 1.58701537 91.48996181 78.43728277]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [ 1.59373558 91.48996091 78.43728186]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [ 1.60045579 91.48996002 78.43728095]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [ 1.607176   91.48995912 78.43728005]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [ 1.6138962  91.48995823 78.43727914]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [ 1.62061641 91.48995733 78.43727823]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [ 1.62733662 91.48995644 78.43727732]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [ 1.63405683 91.48995554 78.43727642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [ 1.64077703 91.48995464 78.43727551]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [ 1.64749724 91.48995375 78.43727461]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [ 1.65421745 91.48995285 78.4372737 ]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [ 1.66093766 91.48995195 78.43727279]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [ 1.66765786 91.48995106 78.43727189]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [ 1.67437807 91.48995016 78.43727098]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [ 1.68109828 91.48994926 78.43727007]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [ 1.68781848 91.48994837 78.43726917]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [ 1.69453869 91.48994747 78.43726826]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [ 1.7012589  91.48994657 78.43726736]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [ 1.7079791  91.48994568 78.43726645]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [ 1.71469931 91.48994478 78.43726554]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [ 1.72141952 91.48994388 78.43726464]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [ 1.72813972 91.48994298 78.43726373]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [ 1.73485993 91.48994209 78.43726283]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [ 1.74158013 91.48994119 78.43726192]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [ 1.74830034 91.48994029 78.43726101]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [ 1.75502055 91.4899394  78.43726011]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [ 1.76174075 91.4899385  78.4372592 ]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [ 1.76846096 91.4899376  78.4372583 ]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [ 1.77518116 91.48993671 78.43725739]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [ 1.78190137 91.48993581 78.43725648]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [ 1.78862157 91.48993491 78.43725558]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [ 1.79534178 91.48993401 78.43725467]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [ 1.80206198 91.48993312 78.43725377]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [ 1.80878219 91.48993222 78.43725286]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [ 1.81550239 91.48993132 78.43725195]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [ 1.8222226  91.48993043 78.43725105]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [ 1.8289428  91.48992953 78.43725014]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [ 1.83566301 91.48992863 78.43724924]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [ 1.84238321 91.48992774 78.43724833]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [ 1.84910342 91.48992684 78.43724742]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [ 1.85582362 91.48992594 78.43724652]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [ 1.86254382 91.48992505 78.43724561]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [ 1.86926403 91.48992415 78.43724471]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [ 1.87598423 91.48992325 78.4372438 ]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [ 1.88270444 91.48992235 78.43724289]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [ 1.88942464 91.48992146 78.43724199]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [ 1.89614484 91.48992056 78.43724108]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [ 1.90286505 91.48991966 78.43724018]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [ 1.90958525 91.48991877 78.43723927]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [ 1.91630545 91.48991787 78.43723836]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [ 1.92302566 91.48991697 78.43723746]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [ 1.92974586 91.48991608 78.43723655]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [ 1.93646606 91.48991518 78.43723565]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [ 1.94318626 91.48991428 78.43723474]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [ 1.94990647 91.48991338 78.43723383]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [ 1.95662667 91.48991249 78.43723293]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [ 1.96334687 91.48991159 78.43723202]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [ 1.97006707 91.48991069 78.43723112]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [ 1.97678728 91.4899098  78.43723021]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [ 1.98350748 91.4899089  78.4372293 ]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [ 1.99022768 91.489908   78.4372284 ]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [ 1.99694788 91.48990711 78.43722749]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [ 2.00366808 91.48990621 78.43722659]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [ 2.01038829 91.48990531 78.43722568]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [ 2.01710849 91.48990442 78.43722477]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [ 2.02382869 91.48990352 78.43722387]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [ 2.03054889 91.48990262 78.43722296]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [ 2.03726909 91.48990172 78.43722206]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [ 2.04398929 91.48990083 78.43722115]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [ 2.05070949 91.48989993 78.43722025]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [ 2.0574297  91.48989903 78.43721934]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [ 2.0641499  91.48989814 78.43721843]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [ 2.0708701  91.48989724 78.43721753]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [ 2.0775903  91.48989634 78.43721662]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [ 2.0843105  91.48989545 78.43721572]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [ 2.0910307  91.48989455 78.43721481]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [ 2.0977509  91.48989365 78.4372139 ]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [ 2.1044711  91.48989275 78.437213  ]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [ 2.1111913  91.48989186 78.43721209]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [ 2.1179115  91.48989096 78.43721119]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [ 2.1246317  91.48989006 78.43721028]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [ 2.1313519  91.48988917 78.43720937]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [ 2.1380721  91.48988827 78.43720847]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [ 2.1447923  91.48988737 78.43720756]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [ 2.1515125  91.48988648 78.43720666]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [ 2.1582327  91.48988558 78.43720575]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [ 2.1649529  91.48988468 78.43720484]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [ 2.1716731  91.48988379 78.43720394]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [ 2.1783933  91.48988289 78.43720303]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [ 2.18511349 91.48988199 78.43720213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [ 2.19183369 91.48988109 78.43720122]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [ 2.19855389 91.4898802  78.43720031]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [ 2.20527409 91.4898793  78.43719941]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [ 2.21199429 91.4898784  78.4371985 ]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [ 2.21871449 91.48987751 78.4371976 ]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [ 2.22543469 91.48987661 78.43719669]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [ 2.23215488 91.48987571 78.43719578]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [ 2.23887508 91.48987482 78.43719488]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [ 2.24559528 91.48987392 78.43719397]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [ 2.25231548 91.48987302 78.43719307]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [ 2.25903568 91.48987212 78.43719216]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [ 2.26575587 91.48987123 78.43719125]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [ 2.27247607 91.48987033 78.43719035]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [ 2.27919627 91.48986943 78.43718944]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [ 2.28591647 91.48986854 78.43718854]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [ 2.29263666 91.48986764 78.43718763]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [ 2.29935686 91.48986674 78.43718672]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [ 2.30607706 91.48986585 78.43718582]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [ 2.31279725 91.48986495 78.43718491]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [ 2.31951745 91.48986405 78.43718401]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [ 2.32623765 91.48986316 78.4371831 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [ 2.33295784 91.48986226 78.43718219]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [ 2.33967804 91.48986136 78.43718129]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [ 2.34639824 91.48986046 78.43718038]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [ 2.35311843 91.48985957 78.43717948]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [ 2.35983863 91.48985867 78.43717857]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [ 2.36655883 91.48985777 78.43717766]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [ 2.37327902 91.48985688 78.43717676]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [ 2.37999922 91.48985598 78.43717585]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [ 2.38671941 91.48985508 78.43717495]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [ 2.39343961 91.48985419 78.43717404]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [ 2.4001598  91.48985329 78.43717313]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [ 2.40688    91.48985239 78.43717223]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [ 2.4136002  91.48985149 78.43717132]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [ 2.42032039 91.4898506  78.43717042]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [ 2.42704059 91.4898497  78.43716951]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [ 2.43376078 91.4898488  78.4371686 ]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [ 2.44048098 91.48984791 78.4371677 ]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [ 2.44720117 91.48984701 78.43716679]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [ 2.45392137 91.48984611 78.43716589]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [ 2.46064156 91.48984522 78.43716498]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [ 2.46736175 91.48984432 78.43716407]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [ 2.47408195 91.48984342 78.43716317]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [ 2.48080214 91.48984253 78.43716226]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [ 2.48752234 91.48984163 78.43716136]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [ 2.49424253 91.48984073 78.43716045]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [ 2.50096273 91.48983983 78.43715954]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [ 2.50768292 91.48983894 78.43715864]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [ 2.51440311 91.48983804 78.43715773]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [ 2.52112331 91.48983714 78.43715683]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [ 2.5278435  91.48983625 78.43715592]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [ 2.53456369 91.48983535 78.43715501]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [ 2.54128389 91.48983445 78.43715411]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [ 2.54800408 91.48983356 78.4371532 ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [ 2.55472427 91.48983266 78.4371523 ]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [ 2.56144447 91.48983176 78.43715139]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [ 2.56816466 91.48983086 78.43715048]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [ 2.57488485 91.48982997 78.43714958]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [ 2.58160504 91.48982907 78.43714867]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [ 2.58832524 91.48982817 78.43714777]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [ 2.59504543 91.48982728 78.43714686]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [ 2.60176562 91.48982638 78.43714595]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [ 2.60848581 91.48982548 78.43714505]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [ 2.61520601 91.48982459 78.43714414]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [ 2.6219262  91.48982369 78.43714324]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [ 2.62864639 91.48982279 78.43714233]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [ 2.63536658 91.4898219  78.43714143]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [ 2.64208677 91.489821   78.43714052]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [ 2.64880697 91.4898201  78.43713961]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [ 2.65552716 91.4898192  78.43713871]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [ 2.66224735 91.48981831 78.4371378 ]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [ 2.66896754 91.48981741 78.4371369 ]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [ 2.67568773 91.48981651 78.43713599]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [ 2.68240792 91.48981562 78.43713508]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [ 2.68912811 91.48981472 78.43713418]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [ 2.6958483  91.48981382 78.43713327]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [ 2.7025685  91.48981293 78.43713237]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [ 2.70928869 91.48981203 78.43713146]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [ 2.71600888 91.48981113 78.43713055]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [ 2.72272907 91.48981023 78.43712965]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [ 2.72944926 91.48980934 78.43712874]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [ 2.73616945 91.48980844 78.43712784]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [ 2.74288964 91.48980754 78.43712693]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [ 2.74960983 91.48980665 78.43712602]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [ 2.75633002 91.48980575 78.43712512]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [ 2.76305021 91.48980485 78.43712421]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [ 2.7697704  91.48980396 78.43712331]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [ 2.77649059 91.48980306 78.4371224 ]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [ 2.78321078 91.48980216 78.43712149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [ 2.78993097 91.48980127 78.43712059]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [ 2.79665115 91.48980037 78.43711968]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [ 2.80337134 91.48979947 78.43711878]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [ 2.81009153 91.48979857 78.43711787]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [ 2.81681172 91.48979768 78.43711696]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [ 2.82353191 91.48979678 78.43711606]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [ 2.8302521  91.48979588 78.43711515]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [ 2.83697229 91.48979499 78.43711425]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [ 2.84369248 91.48979409 78.43711334]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [ 2.85041267 91.48979319 78.43711243]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [ 2.85713285 91.4897923  78.43711153]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [ 2.86385304 91.4897914  78.43711062]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [ 2.87057323 91.4897905  78.43710972]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [ 2.87729342 91.4897896  78.43710881]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [ 2.88401361 91.48978871 78.4371079 ]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [ 2.89073379 91.48978781 78.437107  ]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [ 2.89745398 91.48978691 78.43710609]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [ 2.90417417 91.48978602 78.43710519]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [ 2.91089436 91.48978512 78.43710428]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [ 2.91761454 91.48978422 78.43710337]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [ 2.92433473 91.48978333 78.43710247]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [ 2.93105492 91.48978243 78.43710156]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [ 2.93777511 91.48978153 78.43710066]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [ 2.94449529 91.48978064 78.43709975]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [ 2.95121548 91.48977974 78.43709884]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [ 2.95793567 91.48977884 78.43709794]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [ 2.96465585 91.48977794 78.43709703]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [ 2.97137604 91.48977705 78.43709613]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [ 2.97809623 91.48977615 78.43709522]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [ 2.98481641 91.48977525 78.43709431]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [ 2.9915366  91.48977436 78.43709341]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [ 2.99825678 91.48977346 78.4370925 ]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [ 3.00497697 91.48977256 78.4370916 ]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [ 3.01169716 91.48977167 78.43709069]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [ 3.01841734 91.48977077 78.43708978]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [ 3.02513753 91.48976987 78.43708888]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [ 3.03185771 91.48976897 78.43708797]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [ 3.0385779  91.48976808 78.43708707]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [ 3.04529808 91.48976718 78.43708616]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [ 3.05201827 91.48976628 78.43708525]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [ 3.05873845 91.48976539 78.43708435]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [ 3.06545864 91.48976449 78.43708344]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [ 3.07217882 91.48976359 78.43708254]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [ 3.07889901 91.4897627  78.43708163]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [ 3.08561919 91.4897618  78.43708072]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [ 3.09233938 91.4897609  78.43707982]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [ 3.09905956 91.48976001 78.43707891]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [ 3.10577975 91.48975911 78.43707801]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [ 3.11249993 91.48975821 78.4370771 ]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [ 3.11922012 91.48975731 78.43707619]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [ 3.1259403  91.48975642 78.43707529]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [ 3.13266048 91.48975552 78.43707438]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [ 3.13938067 91.48975462 78.43707348]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [ 3.14610085 91.48975373 78.43707257]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [ 3.15282104 91.48975283 78.43707167]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [ 3.15954122 91.48975193 78.43707076]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [ 3.1662614  91.48975104 78.43706985]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [ 3.17298159 91.48975014 78.43706895]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [ 3.17970177 91.48974924 78.43706804]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [ 3.18642195 91.48974834 78.43706714]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [ 3.19314214 91.48974745 78.43706623]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [ 3.19986232 91.48974655 78.43706532]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [ 3.2065825  91.48974565 78.43706442]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [ 3.21330268 91.48974476 78.43706351]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [ 3.22002287 91.48974386 78.43706261]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [ 3.22674305 91.48974296 78.4370617 ]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [ 3.23346323 91.48974207 78.43706079]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [ 3.24018341 91.48974117 78.43705989]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [ 3.2469036  91.48974027 78.43705898]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [ 3.25362378 91.48973938 78.43705808]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [ 3.26034396 91.48973848 78.43705717]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [ 3.26706414 91.48973758 78.43705626]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [ 3.27378432 91.48973668 78.43705536]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [ 3.2805045  91.48973579 78.43705445]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [ 3.28722469 91.48973489 78.43705355]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [ 3.29394487 91.48973399 78.43705264]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [ 3.30066505 91.4897331  78.43705173]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [ 3.30738523 91.4897322  78.43705083]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [ 3.31410541 91.4897313  78.43704992]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [ 3.32082559 91.48973041 78.43704902]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [ 3.32754577 91.48972951 78.43704811]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [ 3.33426595 91.48972861 78.4370472 ]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [ 3.34098613 91.48972772 78.4370463 ]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [ 3.34770632 91.48972682 78.43704539]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [ 3.3544265  91.48972592 78.43704449]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [ 3.36114668 91.48972502 78.43704358]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [ 3.36786686 91.48972413 78.43704267]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [ 3.37458704 91.48972323 78.43704177]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [ 3.38130722 91.48972233 78.43704086]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [ 3.3880274  91.48972144 78.43703996]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [ 3.39474758 91.48972054 78.43703905]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [ 3.40146776 91.48971964 78.43703814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [ 3.40818794 91.48971875 78.43703724]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [ 3.41490812 91.48971785 78.43703633]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [ 3.42162829 91.48971695 78.43703543]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [ 3.42834847 91.48971605 78.43703452]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [ 3.43506865 91.48971516 78.43703361]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [ 3.44178883 91.48971426 78.43703271]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [ 3.44850901 91.48971336 78.4370318 ]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [ 3.45522919 91.48971247 78.4370309 ]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [ 3.46194937 91.48971157 78.43702999]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [ 3.46866955 91.48971067 78.43702908]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [ 3.47538973 91.48970978 78.43702818]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [ 3.4821099  91.48970888 78.43702727]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [ 3.48883008 91.48970798 78.43702637]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [ 3.49555026 91.48970709 78.43702546]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [ 3.50227044 91.48970619 78.43702455]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [ 3.50899062 91.48970529 78.43702365]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [ 3.5157108  91.48970439 78.43702274]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [ 3.52243097 91.4897035  78.43702184]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [ 3.52915115 91.4897026  78.43702093]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [ 3.53587133 91.4897017  78.43702002]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [ 3.54259151 91.48970081 78.43701912]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [ 3.54931168 91.48969991 78.43701821]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [ 3.55603186 91.48969901 78.43701731]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [ 3.56275204 91.48969812 78.4370164 ]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [ 3.56947222 91.48969722 78.43701549]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [ 3.57619239 91.48969632 78.43701459]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [ 3.58291257 91.48969542 78.43701368]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [ 3.58963275 91.48969453 78.43701278]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [ 3.59635292 91.48969363 78.43701187]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [ 3.6030731  91.48969273 78.43701096]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [ 3.60979328 91.48969184 78.43701006]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [ 3.61651345 91.48969094 78.43700915]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [ 3.62323363 91.48969004 78.43700825]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [ 3.6299538  91.48968915 78.43700734]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [ 3.63667398 91.48968825 78.43700643]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [ 3.64339416 91.48968735 78.43700553]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [ 3.65011433 91.48968646 78.43700462]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [ 3.65683451 91.48968556 78.43700372]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [ 3.66355468 91.48968466 78.43700281]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [ 3.67027486 91.48968376 78.43700191]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [ 3.67699503 91.48968287 78.437001  ]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [ 3.68371521 91.48968197 78.43700009]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [ 3.69043538 91.48968107 78.43699919]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [ 3.69715556 91.48968018 78.43699828]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [ 3.70387573 91.48967928 78.43699738]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [ 3.71059591 91.48967838 78.43699647]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [ 3.71731608 91.48967749 78.43699556]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [ 3.72403626 91.48967659 78.43699466]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [ 3.73075643 91.48967569 78.43699375]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [ 3.73747661 91.48967479 78.43699285]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [ 3.74419678 91.4896739  78.43699194]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [ 3.75091696 91.489673   78.43699103]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [ 3.75763713 91.4896721  78.43699013]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [ 3.7643573  91.48967121 78.43698922]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [ 3.77107748 91.48967031 78.43698832]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [ 3.77779765 91.48966941 78.43698741]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [ 3.78451783 91.48966852 78.4369865 ]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [ 3.791238   91.48966762 78.4369856 ]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [ 3.79795817 91.48966672 78.43698469]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [ 3.80467835 91.48966583 78.43698379]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [ 3.81139852 91.48966493 78.43698288]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [ 3.81811869 91.48966403 78.43698197]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [ 3.82483887 91.48966313 78.43698107]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [ 3.83155904 91.48966224 78.43698016]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [ 3.83827921 91.48966134 78.43697926]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [ 3.84499938 91.48966044 78.43697835]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [ 3.85171956 91.48965955 78.43697744]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [ 3.85843973 91.48965865 78.43697654]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [ 3.8651599  91.48965775 78.43697563]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [ 3.87188007 91.48965686 78.43697473]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [ 3.87860025 91.48965596 78.43697382]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [ 3.88532042 91.48965506 78.43697291]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [ 3.89204059 91.48965416 78.43697201]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [ 3.89876076 91.48965327 78.4369711 ]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [ 3.90548093 91.48965237 78.4369702 ]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [ 3.91220111 91.48965147 78.43696929]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [ 3.91892128 91.48965058 78.43696838]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [ 3.92564145 91.48964968 78.43696748]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [ 3.93236162 91.48964878 78.43696657]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [ 3.93908179 91.48964789 78.43696567]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [ 3.94580196 91.48964699 78.43696476]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [ 3.95252213 91.48964609 78.43696385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [ 3.9592423  91.4896452  78.43696295]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [ 3.96596247 91.4896443  78.43696204]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [ 3.97268265 91.4896434  78.43696114]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [ 3.97940282 91.4896425  78.43696023]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [ 3.98612299 91.48964161 78.43695932]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [ 3.99284316 91.48964071 78.43695842]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [ 3.99956333 91.48963981 78.43695751]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [ 4.0062835  91.48963892 78.43695661]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [ 4.01300367 91.48963802 78.4369557 ]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [ 4.01972384 91.48963712 78.43695479]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [ 4.02644401 91.48963623 78.43695389]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [ 4.03316418 91.48963533 78.43695298]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [ 4.03988435 91.48963443 78.43695208]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [ 4.04660452 91.48963353 78.43695117]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [ 4.05332469 91.48963264 78.43695026]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [ 4.06004485 91.48963174 78.43694936]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [ 4.06676502 91.48963084 78.43694845]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [ 4.07348519 91.48962995 78.43694755]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [ 4.08020536 91.48962905 78.43694664]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [ 4.08692553 91.48962815 78.43694573]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [ 4.0936457  91.48962726 78.43694483]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [ 4.10036587 91.48962636 78.43694392]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [ 4.10708604 91.48962546 78.43694302]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [ 4.11380621 91.48962457 78.43694211]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [ 4.12052637 91.48962367 78.4369412 ]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [ 4.12724654 91.48962277 78.4369403 ]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [ 4.13396671 91.48962187 78.43693939]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [ 4.14068688 91.48962098 78.43693849]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [ 4.14740705 91.48962008 78.43693758]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [ 4.15412721 91.48961918 78.43693667]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [ 4.16084738 91.48961829 78.43693577]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [ 4.16756755 91.48961739 78.43693486]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [ 4.17428772 91.48961649 78.43693396]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [ 4.18100788 91.4896156  78.43693305]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [ 4.18772805 91.4896147  78.43693215]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [ 4.19444822 91.4896138  78.43693124]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [ 4.20116839 91.48961291 78.43693033]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [ 4.20788855 91.48961201 78.43692943]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [ 4.21460872 91.48961111 78.43692852]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [ 4.22132889 91.48961021 78.43692762]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [ 4.22804905 91.48960932 78.43692671]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [ 4.23476922 91.48960842 78.4369258 ]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [ 4.24148939 91.48960752 78.4369249 ]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [ 4.24820955 91.48960663 78.43692399]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [ 4.25492972 91.48960573 78.43692309]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [ 4.26164989 91.48960483 78.43692218]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [ 4.26837005 91.48960394 78.43692127]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [ 4.27509022 91.48960304 78.43692037]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [ 4.28181038 91.48960214 78.43691946]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [ 4.28853055 91.48960124 78.43691856]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [ 4.29525071 91.48960035 78.43691765]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [ 4.30197088 91.48959945 78.43691674]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [ 4.30869105 91.48959855 78.43691584]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [ 4.31541121 91.48959766 78.43691493]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [ 4.32213138 91.48959676 78.43691403]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [ 4.32885154 91.48959586 78.43691312]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [ 4.33557171 91.48959497 78.43691221]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [ 4.34229187 91.48959407 78.43691131]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [ 4.34901204 91.48959317 78.4369104 ]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [ 4.3557322  91.48959228 78.4369095 ]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [ 4.36245236 91.48959138 78.43690859]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [ 4.36917253 91.48959048 78.43690768]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [ 4.37589269 91.48958958 78.43690678]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [ 4.38261286 91.48958869 78.43690587]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [ 4.38933302 91.48958779 78.43690497]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [ 4.39605319 91.48958689 78.43690406]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [ 4.40277335 91.489586   78.43690315]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [ 4.40949351 91.4895851  78.43690225]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [ 4.41621368 91.4895842  78.43690134]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [ 4.42293384 91.48958331 78.43690044]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [ 4.429654   91.48958241 78.43689953]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [ 4.43637417 91.48958151 78.43689862]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [ 4.44309433 91.48958061 78.43689772]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [ 4.44981449 91.48957972 78.43689681]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [ 4.45653466 91.48957882 78.43689591]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [ 4.46325482 91.48957792 78.436895  ]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [ 4.46997498 91.48957703 78.43689409]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [ 4.47669515 91.48957613 78.43689319]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [ 4.48341531 91.48957523 78.43689228]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [ 4.49013547 91.48957434 78.43689138]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [ 4.49685563 91.48957344 78.43689047]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [ 4.5035758  91.48957254 78.43688956]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [ 4.51029596 91.48957165 78.43688866]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [ 4.51701612 91.48957075 78.43688775]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [ 4.52373628 91.48956985 78.43688685]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [ 4.53045644 91.48956895 78.43688594]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [ 4.53717661 91.48956806 78.43688503]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [ 4.54389677 91.48956716 78.43688413]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [ 4.55061693 91.48956626 78.43688322]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [ 4.55733709 91.48956537 78.43688232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [ 4.56405725 91.48956447 78.43688141]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [ 4.57077741 91.48956357 78.4368805 ]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [ 4.57749757 91.48956268 78.4368796 ]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [ 4.58421773 91.48956178 78.43687869]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [ 4.5909379  91.48956088 78.43687779]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [ 4.59765806 91.48955998 78.43687688]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [ 4.60437822 91.48955909 78.43687597]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [ 4.61109838 91.48955819 78.43687507]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [ 4.61781854 91.48955729 78.43687416]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [ 4.6245387  91.4895564  78.43687326]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [ 4.63125886 91.4895555  78.43687235]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [ 4.63797902 91.4895546  78.43687144]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [ 4.64469918 91.48955371 78.43687054]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [ 4.65141934 91.48955281 78.43686963]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [ 4.6581395  91.48955191 78.43686873]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [ 4.66485966 91.48955102 78.43686782]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [ 4.67157982 91.48955012 78.43686692]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [ 4.67829998 91.48954922 78.43686601]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [ 4.68502014 91.48954832 78.4368651 ]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [ 4.6917403  91.48954743 78.4368642 ]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [ 4.69846046 91.48954653 78.43686329]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [ 4.70518062 91.48954563 78.43686239]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [ 4.71190077 91.48954474 78.43686148]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [ 4.71862093 91.48954384 78.43686057]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [ 4.72534109 91.48954294 78.43685967]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [ 4.73206125 91.48954205 78.43685876]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [ 4.73878141 91.48954115 78.43685786]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [ 4.74550157 91.48954025 78.43685695]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [ 4.75222173 91.48953936 78.43685604]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [ 4.75894188 91.48953846 78.43685514]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [ 4.76566204 91.48953756 78.43685423]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [ 4.7723822  91.48953666 78.43685333]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [ 4.77910236 91.48953577 78.43685242]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [ 4.78582252 91.48953487 78.43685151]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [ 4.79254267 91.48953397 78.43685061]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [ 4.79926283 91.48953308 78.4368497 ]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [ 4.80598299 91.48953218 78.4368488 ]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [ 4.81270315 91.48953128 78.43684789]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [ 4.8194233  91.48953039 78.43684698]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [ 4.82614346 91.48952949 78.43684608]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [ 4.83286362 91.48952859 78.43684517]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [ 4.83958378 91.48952769 78.43684427]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [ 4.84630393 91.4895268  78.43684336]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [ 4.85302409 91.4895259  78.43684245]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [ 4.85974425 91.489525   78.43684155]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [ 4.8664644  91.48952411 78.43684064]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [ 4.87318456 91.48952321 78.43683974]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [ 4.87990472 91.48952231 78.43683883]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [ 4.88662487 91.48952142 78.43683792]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [ 4.89334503 91.48952052 78.43683702]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [ 4.90006518 91.48951962 78.43683611]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [ 4.90678534 91.48951873 78.43683521]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [ 4.9135055  91.48951783 78.4368343 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [ 4.92022565 91.48951693 78.43683339]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [ 4.92694581 91.48951603 78.43683249]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [ 4.93366596 91.48951514 78.43683158]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [ 4.94038612 91.48951424 78.43683068]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [ 4.94710627 91.48951334 78.43682977]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [ 4.95382643 91.48951245 78.43682886]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [ 4.96054658 91.48951155 78.43682796]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [ 4.96726674 91.48951065 78.43682705]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [ 4.97398689 91.48950976 78.43682615]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [ 4.98070705 91.48950886 78.43682524]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [ 4.9874272  91.48950796 78.43682433]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [ 4.99414736 91.48950706 78.43682343]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [ 5.00086751 91.48950617 78.43682252]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [ 5.00758767 91.48950527 78.43682162]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [ 5.01430782 91.48950437 78.43682071]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [ 5.02102797 91.48950348 78.4368198 ]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [ 5.02774813 91.48950258 78.4368189 ]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [ 5.03446828 91.48950168 78.43681799]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [ 5.04118844 91.48950079 78.43681709]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [ 5.04790859 91.48949989 78.43681618]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [ 5.05462874 91.48949899 78.43681527]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [ 5.0613489  91.4894981  78.43681437]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [ 5.06806905 91.4894972  78.43681346]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [ 5.0747892  91.4894963  78.43681256]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [ 5.08150936 91.4894954  78.43681165]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [ 5.08822951 91.48949451 78.43681074]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [ 5.09494966 91.48949361 78.43680984]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [ 5.10166982 91.48949271 78.43680893]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [ 5.10838997 91.48949182 78.43680803]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [ 5.11511012 91.48949092 78.43680712]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [ 5.12183027 91.48949002 78.43680622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [ 5.12855043 91.48948913 78.43680531]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [ 5.13527058 91.48948823 78.4368044 ]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [ 5.14199073 91.48948733 78.4368035 ]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [ 5.14871088 91.48948643 78.43680259]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [ 5.15543103 91.48948554 78.43680169]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [ 5.16215119 91.48948464 78.43680078]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [ 5.16887134 91.48948374 78.43679987]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [ 5.17559149 91.48948285 78.43679897]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [ 5.18231164 91.48948195 78.43679806]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [ 5.18903179 91.48948105 78.43679716]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [ 5.19575194 91.48948016 78.43679625]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [ 5.2024721  91.48947926 78.43679534]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [ 5.20919225 91.48947836 78.43679444]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [ 5.2159124  91.48947747 78.43679353]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [ 5.22263255 91.48947657 78.43679263]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [ 5.2293527  91.48947567 78.43679172]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [ 5.23607285 91.48947477 78.43679081]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [ 5.242793   91.48947388 78.43678991]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [ 5.24951315 91.48947298 78.436789  ]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [ 5.2562333  91.48947208 78.4367881 ]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [ 5.26295345 91.48947119 78.43678719]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [ 5.2696736  91.48947029 78.43678628]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [ 5.27639375 91.48946939 78.43678538]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [ 5.2831139  91.4894685  78.43678447]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [ 5.28983405 91.4894676  78.43678357]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [ 5.2965542  91.4894667  78.43678266]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [ 5.30327435 91.48946581 78.43678175]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [ 5.3099945  91.48946491 78.43678085]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [ 5.31671465 91.48946401 78.43677994]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [ 5.3234348  91.48946311 78.43677904]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [ 5.33015495 91.48946222 78.43677813]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [ 5.3368751  91.48946132 78.43677722]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [ 5.34359525 91.48946042 78.43677632]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [ 5.3503154  91.48945953 78.43677541]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [ 5.35703555 91.48945863 78.43677451]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [ 5.36375569 91.48945773 78.4367736 ]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [ 5.37047584 91.48945684 78.43677269]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [ 5.37719599 91.48945594 78.43677179]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [ 5.38391614 91.48945504 78.43677088]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [ 5.39063629 91.48945414 78.43676998]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [ 5.39735644 91.48945325 78.43676907]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [ 5.40407658 91.48945235 78.43676816]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [ 5.41079673 91.48945145 78.43676726]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [ 5.41751688 91.48945056 78.43676635]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [ 5.42423703 91.48944966 78.43676545]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [ 5.43095718 91.48944876 78.43676454]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [ 5.43767732 91.48944787 78.43676363]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [ 5.44439747 91.48944697 78.43676273]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [ 5.45111762 91.48944607 78.43676182]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [ 5.45783777 91.48944518 78.43676092]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [ 5.46455791 91.48944428 78.43676001]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [ 5.47127806 91.48944338 78.4367591 ]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [ 5.47799821 91.48944248 78.4367582 ]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [ 5.48471835 91.48944159 78.43675729]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [ 5.4914385  91.48944069 78.43675639]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [ 5.49815865 91.48943979 78.43675548]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [ 5.50487879 91.4894389  78.43675457]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [ 5.51159894 91.489438   78.43675367]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [ 5.51831909 91.4894371  78.43675276]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [ 5.52503923 91.48943621 78.43675186]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [ 5.53175938 91.48943531 78.43675095]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [ 5.53847952 91.48943441 78.43675004]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [ 5.54519967 91.48943351 78.43674914]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [ 5.55191982 91.48943262 78.43674823]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [ 5.55863996 91.48943172 78.43674733]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [ 5.56536011 91.48943082 78.43674642]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [ 5.57208025 91.48942993 78.43674552]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [ 5.5788004  91.48942903 78.43674461]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [ 5.58552054 91.48942813 78.4367437 ]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [ 5.59224069 91.48942724 78.4367428 ]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [ 5.59896083 91.48942634 78.43674189]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [ 5.60568098 91.48942544 78.43674099]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [ 5.61240112 91.48942455 78.43674008]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [ 5.61912127 91.48942365 78.43673917]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [ 5.62584141 91.48942275 78.43673827]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [ 5.63256156 91.48942185 78.43673736]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [ 5.6392817  91.48942096 78.43673646]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [ 5.64600184 91.48942006 78.43673555]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [ 5.65272199 91.48941916 78.43673464]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [ 5.65944213 91.48941827 78.43673374]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ 5.66616228 91.48941737 78.43673283]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ 5.67288242 91.48941647 78.43673193]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ 5.67960256 91.48941558 78.43673102]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ 5.68632271 91.48941468 78.43673011]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ 5.69304285 91.48941378 78.43672921]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ 5.69976299 91.48941289 78.4367283 ]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ 5.70648314 91.48941199 78.4367274 ]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ 5.71320328 91.48941109 78.43672649]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ 5.71992342 91.48941019 78.43672558]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ 5.72664357 91.4894093  78.43672468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ 5.73336371 91.4894084  78.43672377]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ 5.74008385 91.4894075  78.43672287]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ 5.746804   91.48940661 78.43672196]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ 5.75352414 91.48940571 78.43672105]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ 5.76024428 91.48940481 78.43672015]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ 5.76696442 91.48940392 78.43671924]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ 5.77368457 91.48940302 78.43671834]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ 5.78040471 91.48940212 78.43671743]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ 5.78712485 91.48940122 78.43671652]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ 5.79384499 91.48940033 78.43671562]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ 5.80056513 91.48939943 78.43671471]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ 5.80728527 91.48939853 78.43671381]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ 5.81400542 91.48939764 78.4367129 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ 5.82072556 91.48939674 78.43671199]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ 5.8274457  91.48939584 78.43671109]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ 5.83416584 91.48939495 78.43671018]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ 5.84088598 91.48939405 78.43670928]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ 5.84760612 91.48939315 78.43670837]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ 5.85432626 91.48939226 78.43670746]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ 5.8610464  91.48939136 78.43670656]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ 5.86776655 91.48939046 78.43670565]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ 5.87448669 91.48938956 78.43670475]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ 5.88120683 91.48938867 78.43670384]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ 5.88792697 91.48938777 78.43670293]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ 5.89464711 91.48938687 78.43670203]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ 5.90136725 91.48938598 78.43670112]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ 5.90808739 91.48938508 78.43670022]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ 5.91480753 91.48938418 78.43669931]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ 5.92152767 91.48938329 78.4366984 ]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ 5.92824781 91.48938239 78.4366975 ]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ 5.93496795 91.48938149 78.43669659]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ 5.94168809 91.48938059 78.43669569]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ 5.94840823 91.4893797  78.43669478]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ 5.95512837 91.4893788  78.43669387]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ 5.9618485  91.4893779  78.43669297]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ 5.96856864 91.48937701 78.43669206]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ 5.97528878 91.48937611 78.43669116]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ 5.98200892 91.48937521 78.43669025]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ 5.98872906 91.48937432 78.43668934]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ 5.9954492  91.48937342 78.43668844]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ 6.00216934 91.48937252 78.43668753]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ 6.00888948 91.48937163 78.43668663]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ 6.01560962 91.48937073 78.43668572]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ 6.02232975 91.48936983 78.43668482]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ 6.02904989 91.48936893 78.43668391]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ 6.03577003 91.48936804 78.436683  ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ 6.04249017 91.48936714 78.4366821 ]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ 6.04921031 91.48936624 78.43668119]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ 6.05593044 91.48936535 78.43668029]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ 6.06265058 91.48936445 78.43667938]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ 6.06937072 91.48936355 78.43667847]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ 6.07609086 91.48936266 78.43667757]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ 6.08281099 91.48936176 78.43667666]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ 6.08953113 91.48936086 78.43667576]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ 6.09625127 91.48935997 78.43667485]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ 6.1029714  91.48935907 78.43667394]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ 6.10969154 91.48935817 78.43667304]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ 6.11641168 91.48935727 78.43667213]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ 6.12313182 91.48935638 78.43667123]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ 6.12985195 91.48935548 78.43667032]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ 6.13657209 91.48935458 78.43666941]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ 6.14329222 91.48935369 78.43666851]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ 6.15001236 91.48935279 78.4366676 ]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ 6.1567325  91.48935189 78.4366667 ]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ 6.16345263 91.489351   78.43666579]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ 6.17017277 91.4893501  78.43666488]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ 6.17689291 91.4893492  78.43666398]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ 6.18361304 91.4893483  78.43666307]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ 6.19033318 91.48934741 78.43666217]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ 6.19705331 91.48934651 78.43666126]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ 6.20377345 91.48934561 78.43666035]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ 6.21049358 91.48934472 78.43665945]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ 6.21721372 91.48934382 78.43665854]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ 6.22393385 91.48934292 78.43665764]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ 6.23065399 91.48934203 78.43665673]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ 6.23737412 91.48934113 78.43665582]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ 6.24409426 91.48934023 78.43665492]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ 6.25081439 91.48933934 78.43665401]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ 6.25753453 91.48933844 78.43665311]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ 6.26425466 91.48933754 78.4366522 ]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ 6.2709748  91.48933664 78.43665129]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ 6.27769493 91.48933575 78.43665039]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ 6.28441506 91.48933485 78.43664948]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ 6.2911352  91.48933395 78.43664858]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ 6.29785533 91.48933306 78.43664767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ 6.30457547 91.48933216 78.43664676]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ 6.3112956  91.48933126 78.43664586]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ 6.31801573 91.48933037 78.43664495]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ 6.32473587 91.48932947 78.43664405]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ 6.331456   91.48932857 78.43664314]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ 6.33817613 91.48932768 78.43664223]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ 6.34489627 91.48932678 78.43664133]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ 6.3516164  91.48932588 78.43664042]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ 6.35833653 91.48932498 78.43663952]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ 6.36505667 91.48932409 78.43663861]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ 6.3717768  91.48932319 78.4366377 ]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ 6.37849693 91.48932229 78.4366368 ]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ 6.38521706 91.4893214  78.43663589]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ 6.3919372  91.4893205  78.43663499]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ 6.39865733 91.4893196  78.43663408]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ 6.40537746 91.48931871 78.43663317]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ 6.41209759 91.48931781 78.43663227]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ 6.41881773 91.48931691 78.43663136]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ 6.42553786 91.48931601 78.43663046]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ 6.43225799 91.48931512 78.43662955]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ 6.43897812 91.48931422 78.43662865]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ 6.44569825 91.48931332 78.43662774]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ 6.45241838 91.48931243 78.43662683]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ 6.45913852 91.48931153 78.43662593]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ 6.46585865 91.48931063 78.43662502]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ 6.47257878 91.48930974 78.43662412]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ 6.47929891 91.48930884 78.43662321]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ 6.48601904 91.48930794 78.4366223 ]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ 6.49273917 91.48930705 78.4366214 ]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ 6.4994593  91.48930615 78.43662049]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ 6.50617943 91.48930525 78.43661959]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ 6.51289956 91.48930435 78.43661868]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ 6.51961969 91.48930346 78.43661777]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ 6.52633982 91.48930256 78.43661687]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ 6.53305995 91.48930166 78.43661596]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ 6.53978008 91.48930077 78.43661506]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ 6.54650021 91.48929987 78.43661415]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ 6.55322034 91.48929897 78.43661324]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ 6.55994047 91.48929808 78.43661234]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ 6.5666606  91.48929718 78.43661143]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ 6.57338073 91.48929628 78.43661053]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ 6.58010086 91.48929538 78.43660962]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ 6.58682099 91.48929449 78.43660871]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ 6.59354112 91.48929359 78.43660781]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ 6.60026125 91.48929269 78.4366069 ]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ 6.60698138 91.4892918  78.436606  ]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ 6.61370151 91.4892909  78.43660509]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ 6.62042164 91.48929    78.43660418]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ 6.62714177 91.48928911 78.43660328]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ 6.63386189 91.48928821 78.43660237]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ 6.64058202 91.48928731 78.43660147]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ 6.64730215 91.48928642 78.43660056]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ 6.65402228 91.48928552 78.43659965]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ 6.66074241 91.48928462 78.43659875]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ 6.66746254 91.48928372 78.43659784]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ 6.67418266 91.48928283 78.43659694]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ 6.68090279 91.48928193 78.43659603]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ 6.68762292 91.48928103 78.43659512]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ 6.69434305 91.48928014 78.43659422]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ 6.70106318 91.48927924 78.43659331]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ 6.7077833  91.48927834 78.43659241]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ 6.71450343 91.48927745 78.4365915 ]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ 6.72122356 91.48927655 78.43659059]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ 6.72794368 91.48927565 78.43658969]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ 6.73466381 91.48927476 78.43658878]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ 6.74138394 91.48927386 78.43658788]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ 6.74810407 91.48927296 78.43658697]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ 6.75482419 91.48927206 78.43658606]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ 6.76154432 91.48927117 78.43658516]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ 6.76826445 91.48927027 78.43658425]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ 6.77498457 91.48926937 78.43658335]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ 6.7817047  91.48926848 78.43658244]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ 6.78842482 91.48926758 78.43658153]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ 6.79514495 91.48926668 78.43658063]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ 6.80186508 91.48926579 78.43657972]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ 6.8085852  91.48926489 78.43657882]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ 6.81530533 91.48926399 78.43657791]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ 6.82202545 91.48926309 78.43657701]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ 6.82874558 91.4892622  78.4365761 ]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ 6.8354657  91.4892613  78.43657519]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ 6.84218583 91.4892604  78.43657429]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ 6.84890595 91.48925951 78.43657338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ 6.85562608 91.48925861 78.43657248]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ 6.86234621 91.48925771 78.43657157]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ 6.86906633 91.48925682 78.43657066]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ 6.87578645 91.48925592 78.43656976]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ 6.88250658 91.48925502 78.43656885]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ 6.8892267  91.48925413 78.43656795]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ 6.89594683 91.48925323 78.43656704]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ 6.90266695 91.48925233 78.43656613]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ 6.90938708 91.48925143 78.43656523]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ 6.9161072  91.48925054 78.43656432]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ 6.92282733 91.48924964 78.43656342]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ 6.92954745 91.48924874 78.43656251]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ 6.93626757 91.48924785 78.4365616 ]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ 6.9429877  91.48924695 78.4365607 ]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ 6.94970782 91.48924605 78.43655979]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ 6.95642794 91.48924516 78.43655889]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ 6.96314807 91.48924426 78.43655798]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ 6.96986819 91.48924336 78.43655707]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ 6.97658831 91.48924247 78.43655617]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ 6.98330844 91.48924157 78.43655526]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ 6.99002856 91.48924067 78.43655436]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ 6.99674868 91.48923977 78.43655345]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ 7.00346881 91.48923888 78.43655254]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ 7.01018893 91.48923798 78.43655164]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ 7.01690905 91.48923708 78.43655073]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ 7.02362917 91.48923619 78.43654983]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ 7.0303493  91.48923529 78.43654892]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ 7.03706942 91.48923439 78.43654801]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ 7.04378954 91.4892335  78.43654711]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ 7.05050966 91.4892326  78.4365462 ]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ 7.05722978 91.4892317  78.4365453 ]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ 7.06394991 91.4892308  78.43654439]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ 7.07067003 91.48922991 78.43654348]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ 7.07739015 91.48922901 78.43654258]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ 7.08411027 91.48922811 78.43654167]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ 7.09083039 91.48922722 78.43654077]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ 7.09755051 91.48922632 78.43653986]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ 7.10427064 91.48922542 78.43653895]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ 7.11099076 91.48922453 78.43653805]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ 7.11771088 91.48922363 78.43653714]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ 7.124431   91.48922273 78.43653624]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ 7.13115112 91.48922184 78.43653533]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ 7.13787124 91.48922094 78.43653442]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ 7.14459136 91.48922004 78.43653352]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ 7.15131148 91.48921914 78.43653261]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ 7.1580316  91.48921825 78.43653171]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ 7.16475172 91.48921735 78.4365308 ]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ 7.17147184 91.48921645 78.43652989]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ 7.17819196 91.48921556 78.43652899]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ 7.18491208 91.48921466 78.43652808]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ 7.1916322  91.48921376 78.43652718]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ 7.19835232 91.48921287 78.43652627]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ 7.20507244 91.48921197 78.43652536]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ 7.21179256 91.48921107 78.43652446]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ 7.21851268 91.48921018 78.43652355]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ 7.2252328  91.48920928 78.43652265]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ 7.23195292 91.48920838 78.43652174]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ 7.23867304 91.48920748 78.43652084]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ 7.24539316 91.48920659 78.43651993]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ 7.25211328 91.48920569 78.43651902]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ 7.25883339 91.48920479 78.43651812]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ 7.26555351 91.4892039  78.43651721]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ 7.27227363 91.489203   78.43651631]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ 7.27899375 91.4892021  78.4365154 ]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ 7.28571387 91.48920121 78.43651449]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ 7.29243399 91.48920031 78.43651359]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ 7.2991541  91.48919941 78.43651268]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ 7.30587422 91.48919851 78.43651178]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ 7.31259434 91.48919762 78.43651087]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ 7.31931446 91.48919672 78.43650996]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ 7.32603458 91.48919582 78.43650906]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ 7.33275469 91.48919493 78.43650815]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ 7.33947481 91.48919403 78.43650725]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ 7.34619493 91.48919313 78.43650634]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ 7.35291505 91.48919224 78.43650543]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ 7.35963516 91.48919134 78.43650453]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ 7.36635528 91.48919044 78.43650362]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ 7.3730754  91.48918955 78.43650272]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ 7.37979551 91.48918865 78.43650181]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ 7.38651563 91.48918775 78.4365009 ]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ 7.39323575 91.48918685 78.4365    ]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ 7.39995586 91.48918596 78.43649909]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ 7.40667598 91.48918506 78.43649819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ 7.4133961  91.48918416 78.43649728]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ 7.42011621 91.48918327 78.43649637]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ 7.42683633 91.48918237 78.43649547]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ 7.43355644 91.48918147 78.43649456]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ 7.44027656 91.48918058 78.43649366]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ 7.44699668 91.48917968 78.43649275]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ 7.45371679 91.48917878 78.43649184]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ 7.46043691 91.48917789 78.43649094]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ 7.46715702 91.48917699 78.43649003]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ 7.47387714 91.48917609 78.43648913]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ 7.48059725 91.48917519 78.43648822]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ 7.48731737 91.4891743  78.43648731]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ 7.49403748 91.4891734  78.43648641]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ 7.5007576 91.4891725 78.4364855]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ 7.50747771 91.48917161 78.4364846 ]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ 7.51419783 91.48917071 78.43648369]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ 7.52091794 91.48916981 78.43648278]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ 7.52763806 91.48916892 78.43648188]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ 7.53435817 91.48916802 78.43648097]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ 7.54107829 91.48916712 78.43648007]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ 7.5477984  91.48916622 78.43647916]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ 7.55451851 91.48916533 78.43647825]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ 7.56123863 91.48916443 78.43647735]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ 7.56795874 91.48916353 78.43647644]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ 7.57467886 91.48916264 78.43647554]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ 7.58139897 91.48916174 78.43647463]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ 7.58811908 91.48916084 78.43647372]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ 7.5948392  91.48915995 78.43647282]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ 7.60155931 91.48915905 78.43647191]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ 7.60827942 91.48915815 78.43647101]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ 7.61499954 91.48915726 78.4364701 ]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ 7.62171965 91.48915636 78.4364692 ]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ 7.62843976 91.48915546 78.43646829]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ 7.63515987 91.48915456 78.43646738]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ 7.64187999 91.48915367 78.43646648]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ 7.6486001  91.48915277 78.43646557]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ 7.65532021 91.48915187 78.43646467]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ 7.66204033 91.48915098 78.43646376]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ 7.66876044 91.48915008 78.43646285]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ 7.67548055 91.48914918 78.43646195]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ 7.68220066 91.48914829 78.43646104]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ 7.68892077 91.48914739 78.43646014]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ 7.69564089 91.48914649 78.43645923]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ 7.702361   91.4891456  78.43645832]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ 7.70908111 91.4891447  78.43645742]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ 7.71580122 91.4891438  78.43645651]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ 7.72252133 91.4891429  78.43645561]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ 7.72924144 91.48914201 78.4364547 ]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ 7.73596155 91.48914111 78.43645379]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ 7.74268167 91.48914021 78.43645289]\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix, \n",
    "                                                             output, inital_weights=initial_weights,\n",
    "                                                             setp_size=step_size, l2_penalty=1e11, \n",
    "                                                             max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762538419109621.0 274067693220835.97 500404246871243.5\n"
     ]
    }
   ],
   "source": [
    "inital_weights = np.ones(3)\n",
    "multiple_prediction_all_zeros = predict_output(test_feature_matrix, inital_weights)\n",
    "multiple_prediction_no_penalty = predict_output(test_feature_matrix, multiple_weights_0_penalty)\n",
    "multiple_prediction_high_penalty = predict_output(test_feature_matrix, multiple_weights_high_penalty)\n",
    "\n",
    "multiple_RSS_all_zeros = sum((test_output - multiple_prediction_all_zeros)**2)\n",
    "multiple_RSS_no_penalty = sum((test_output - multiple_prediction_no_penalty)**2)\n",
    "multiple_RSS_high_penalty = sum((test_output - multiple_prediction_high_penalty)**2)\n",
    "print(multiple_RSS_all_zeros, multiple_RSS_no_penalty, multiple_RSS_high_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test error of the first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-77465.73529944709"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error1 = (test_output - multiple_prediction_no_penalty)[0]\n",
    "error1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39545.90067311947"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error1_penalty = (test_output - multiple_prediction_high_penalty)[0]\n",
    "error1_penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
